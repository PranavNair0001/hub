{
  "properties": {
    "doc.Deduplicate-batchaggregator": "# Deduplicate\n\n\nDescription\n-----------\nDe-duplicates records either using one or more fields or by using the record as a whole. Additionally, it supports logically\nchoosing a record out of the duplicate records based on a filter field. Supported logical functions are `max`, `min`,\n`first`, and `last`.\n\nUse Case\n--------\nThe aggregator is used when you want to filter out duplicates in the input in a predictable way.\n\nProperties\n----------\n**uniqueFields:** An optional comma-separated list of fields on which to perform the deduplication. If none given, each \nrecord will be considered as a whole for deduplication. For example, if the input contain records with fields `fname`, \n`lname`, `item`, and `cost`, and we want to deduplicate the records by name, then this property should be set to \n`fname,lname`.\n\n**filterOperation:** An optional property that can be set to predictably choose one or more records from the set of records\nthat needs to be deduplicated. This property takes in a field name and the logical operation that needs to be performed\non that field on the set of records. The syntax is `field:function`. For example, if we want to choose the record with\nmaximum cost for the records with schema `fname`, `lname`, `item`, `cost`, then this field should be set as `cost:max`.\nSupported functions are `first`, `last`, `max`, and `min`. Note that only one pair of field and function is allowed.\nIf this property is not set, one random record will be chosen from the group of 'duplicate' records.\n\n**numPartitions:** An optional number of partitions to use when grouping unique fields. If not specified, the execution\nframework will decide on the number to use.\n\nExample\n-------\nThis example deduplicates records by their `fname` and `lname` fields. Then, it chooses one record out of the\nduplicates based on the `cost` field. Since the function specified is `max`, the record with the maximum value in the\n`cost` field is chosen out of each set of duplicate records.\n\n```json\n    {\n        \"name\": \"Deduplicate\",\n        \"type\": \"batchaggregator\",\n        \"properties\": {\n            \"uniqueFields\": \"fname,lname\",\n            \"filterOperation\": \"cost:max\"\n        }\n    }\n```\n\nFor example, suppose the aggregator receives input records where each record represents a purchase:\n\n| fname  | lname   | cost   |  zipcode |\n| ------ | ------- | ------ | -------- |\n| bob    | smith   | 50.23  |  12345   |\n| bob    | jones   | 30.64  |  23456   |\n| alice  | smith   | 1.50   |  34567   |\n| bob    | smith   | 0.50   |  45678   |\n| alice  | smith   | 30.21  |  56789   |\n| alice  | jones   | 500.93 |  67890   |\n    \n\nOutput records will contain one record for each `fname,lname` combination that has the maximum `cost`:\n\n| fname  | lname   | cost   |  zipcode |\n| ------ | ------- | ------ | -------- |\n| bob    | smith   | 50.23  |  12345   |\n| bob    | jones   | 30.64  |  23456   |\n| alice  | smith   | 30.21  |  56789   |\n| alice  | jones   | 500.93 |  67890   |\n",
    "doc.SnapshotParquet-batchsink": "# Snapshot Parquet Batch Sink\n\n\nDescription\n-----------\nA batch sink for a PartitionedFileSet that writes snapshots of data as a new\npartition. Data is written in Parquet format. A corresponding SnapshotParquet source\ncan be used to read only the most recently written snapshot.\n\n\nUse Case\n--------\nThis sink is used whenever you want access to a PartitionedFileSet containing exactly the\nmost recent run's data in Parquet format. For example, you might want to create daily\nsnapshots of a database by reading the entire contents of a table, writing to this sink,\nand then other programs can analyze the contents of the specified file.\n\n\nProperties\n----------\n**name:** Name of the PartitionedFileSet to which records are written.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**schema:** The Parquet schema of the record being written to the sink as a JSON object. (Macro-enabled)\n\n**basePath:** Base path for the PartitionedFileSet. Defaults to the name of the dataset. (Macro-enabled)\n\n**fileProperties:** Advanced feature to specify any additional properties that should be used with the sink,\nspecified as a JSON object of string to string. These properties are set on the dataset if one is created.\nThe properties are also passed to the dataset at runtime as arguments. (Macro-enabled)\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, GZip.\n\n\nExample\n-------\nThis example will write to a PartitionedFileSet named 'users'. It will write data in Parquet format\ncompressed using Snappy compressions using the given schema. Every time the pipeline runs, the most recent run will be stored in\na new partition in the PartitionedFileSet:\n\n    {\n        \"name\": \"SnapshotParquet\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }",
    "doc.SSH-action": "# SSH Action\n\n\nDescription\n-----------\nEstablishes an SSH connection with remote machine to execute command on that machine.\n\n\nUse Case\n--------\nThis action can be used when data needs to be copied to HDFS from a remote server before the pipeline starts.\nIt can also be used to move data for archiving before the pipeline starts.\n\n\nProperties\n----------\n\n**host:** The host name of the remote machine where the command needs to be executed.\n\n**port:** The port used to make SSH connection. Defaults to 22.\n\n**user:** The username used to connect to host.\n\n**privateKey:** The private key to be used to perform the secure shell action. Users can also specify a macro that will\npull the private key from the secure key management store in CDAP such as ${secure(myPrivateKey)}.\n\n**passphrase:** Passphrase used to decrypt the provided private key in \"privateKey\".\n\n**command:** The command to be executed on the remote host. This includes the filepath of the script and any arguments\nneeding to be passed.\n\n**outputKey:** The key used to store the output of the command run by the action.\nPlugins that run at later stages in the pipeline can retrieve the command's output using this key through\nmacro substitution (e.g. ${outputKey} where \"outputKey\" is the key specified). Defaults to \"sshOutput\".\n\nExample\n-------\nThis example runs a script on demo@example.com:\n\n    {\n        \"name\": \"SSHAction\",\n          \"plugin\": {\n            \"name\": \"SSH\",\n            \"type\": \"action\",\n            \"label\": \"SSH\",\n            \"artifact\": {\n              \"name\": \"core-plugins\",\n              \"version\": \"1.4.0-SNAPSHOT\",\n              \"scope\": \"SYSTEM\"\n          },\n          \"properties\": {\n              \"host\": \"example.com\",\n              \"port\": \"22\",\n              \"user\": \"demo\",\n              \"privateKey\": \"${secure(myPrivateKey)}\",\n              \"passphrase\": \"pass\",\n              \"command\": \"~/scripts/remoteScript.sh\",\n              \"outputKey\": \"myActionOutput\"\n            }\n          }\n    }\n",
    "doc.Cube-batchsink": "# Cube Batch Sink\n\n\nDescription\n-----------\nBatch sink that writes data to a Cube dataset.\n\nBatchCubeSink takes a StructuredRecord in, maps it to a CubeFact, and writes it to\nthe Cube dataset identified by the name property.\n\nIf the Cube dataset does not exist, it will be created using properties provided with this\nsink.\n\n\nUse Case\n--------\nA BatchCubeSink is used to write data into a Cube from batch-enabled data sources to allow\nfurther data analysis. For example, you can periodically upload data from a\nPartitionedFileSet into a Cube to perform complex data queries across multiple dimensions\nand aggregated measurements.\n\n\nProperties\n----------\n**name:** Name of the Cube dataset. If the Cube does not already exist, one will be created.\n\n**dataset.cube.resolutions:** Aggregation resolutions to be used if a\nnew Cube dataset needs to be created. See [Cube dataset configuration details] for more information.\n\n**dataset.cube.aggregations:** Provides cube aggregation dataset properties to be used\nif a new Cube dataset needs to be created; provided as a collection of aggregation-groups.\nEach aggregation group is identified by a unique name and value is a collection of fields.\nexample, if aggregations are desired on fields ``abc`` and ``xyz``, the\nproperty can have the key ``agg1`` and value ``abc,xyz``:\nSee [Cube dataset configuration details] for more information.\n[Cube dataset configuration details]: http://docs.cdap.io/cdap/current/en/developer-manual/building-blocks/datasets/cube.html\n\n**dataset.cube.properties:** Provides additional cube dataset properties if needed.\n\n**cubeFact.timestamp.field:** Name of the StructuredRecord field that contains the\ntimestamp to be used in the CubeFact. If not provided, the current time of the record\nprocessing will be used as the CubeFact timestamp.\n\nName of the StructuredRecord's field that contains the timestamp to be used in CubeFact.\nIf not provided, the current time of the record processing will be used as a CubeFact timestamp.\n\n**cubeFact.timestamp.format:** Format of the value of timestamp field; example: \"HH:mm:ss\" (used if\n``cubeFact.timestamp.field`` is provided).\n\n**cubeFact.measurements:** Measurements to be extracted from StructuredRecord to be used\nin CubeFact. Supports collection of measurements and requires at least one measurement to be provided.\nEach measurement has measurement-name and measurement-type. Currently supported measurement types are COUNTER, GAUGE.\nFor example, to use the 'price' field as a\nmeasurement of type gauge, and the 'quantity' field as a measurement of type counter, you would add two measurements\none measurement with name `price` and type `GAUGE, second measurement with name `quantity` and type `COUNTER`.\n\n\nExample\n-------\nThis configuration specifies writing data into a Cube dataset named \"myCube\"; it provides\ndataset properties (``dataset.*``) for creating a new dataset, if one with the given name\ndoesn't exist; it then configures measurements to aggregate and specifies the timestamp\nformat for facts being written into the Cube:\n\n    {\n        \"name\": \"Cube\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"myCube\",\n            \"dataset.cube.resolutions\": \"1,60,3600\",\n            \"dataset.cube.properties\": \"{\n                \\\"dataset.cube.aggregation.byName.dimensions\\\": \\\"name\\\",\n                \\\"dataset.cube.aggregation.byNameByZip.dimensions\\\": \\\"name,zip\\\"\n            }\",\n            \"cubeFact.timestamp.field\": \"ts\",\n            \"cubeFact.timestamp.format\": \"MM/dd/yyyy HH:mm:ss\",\n            \"cubeFact.measurements\": \"{\n                \\\"cubeFact.measurement.price\\\": \\\"GAUGE\\\",\n                \\\"cubeFact.measurement.quantity\\\": \\\"COUNTER\\\"\n            }\"\n        }\n    }\n\nOnce the data is there, you could run queries using an ``AbstractCubeHttpHandler``. (You'd\nneed to deploy an application containing a service using it.) A sample query:\n\n    {\n        \"aggregation\": \"byName\",\n        \"startTs\": 1323370200,\n        \"endTs\":   1523398198,\n        \"measurements\": {\"price\": \"MAX\"},\n        \"resolution\": 1,\n        \"dimensionValues\": {},\n        \"groupByDimensions\": [\"name\"],\n        \"limit\": 1000\n    }\n\nExample result:\n\n    [\n        {\n            \"measureName\": \"price\",\n            \"dimensionValues\": {\n                \"name\": \"user1\"\n            },\n            \"timeValues\": [\n                {\n                    \"timestamp\": 1323370201,\n                    \"value\": 100\n                },\n                {\n                    \"timestamp\": 1323370210,\n                    \"value\": 360\n                }\n            ]\n        },\n        {\n            \"measureName\": \"price\",\n            \"dimensionValues\": {\n                \"name\": \"user2\"\n            },\n            \"timeValues\": [\n                {\n                    \"timestamp\": 1323370201,\n                    \"value\": 200\n                },\n                {\n                    \"timestamp\": 1323370210,\n                    \"value\": 160\n                }\n            ]\n        }\n    ]\n",
    "doc.TPFSParquet-batchsource": "# TimePartitionedFileSet Parquet Batch Source\n\n\nDescription\n-----------\nReads from a TimePartitionedFileSet whose data is in Parquet format.\n\n\nUse Case\n--------\nThe source is used when you need to read partitions of a TimePartitionedFileSet.\nFor example, suppose there is an application that ingests data by writing to a TimePartitionedFileSet,\nwhere arrival time of the data is used as the partition key. You may want to create a pipeline that\nreads the newly-arrived files, performs data validation and cleansing, and then writes to a Table.\n\n\nProperties\n----------\n**Dataset Name:** Name of the TimePartitionedFileSet from which the records are to be read from. (Macro-enabled)\n\n**Dataset Base Path:** Base path for the TimePartitionedFileSet. Defaults to the name of the\ndataset. (Macro-enabled)\n\n**Duration:** Size of the time window to read with each run of the pipeline. The format is\nexpected to be a number followed by an 's', 'm', 'h', or 'd' (specifying the time unit), with\n's' for seconds, 'm' for minutes, 'h' for hours, and 'd' for days. For example, a value of\n'5m' means each run of the pipeline will read 5 minutes of events from the TPFS source. (Macro-enabled)\n\n**Delay:** Optional delay for reading from TPFS source. The value must be of the same\nformat as the duration value. For example, a duration of '5m' and a delay of '10m' means\neach run of the pipeline will read events for 5 minutes of data from 15 minutes before its logical\nstart time to 10 minutes before its logical start time. The default value is 0. (Macro-enabled)\n\n**Schema:** The Parquet schema of the record being read from the source as a JSON Object.\n\n\nExample\n-------\nThis example reads from a TimePartitionedFileSet named 'webactivity', assuming the underlying\nfiles are in Parquet format:\n\n```json\n{\n    \"name\": \"TPFSParquet\",\n    \"type\": \"batchsource\",\n    \"properties\": {\n        \"name\": \"webactivity\",\n        \"duration\": \"5m\",\n        \"delay\": \"1m\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"webactivity\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"userid\\\",\\\"type\\\":\\\"long\\\"},\n                {\\\"name\\\":\\\"action\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"}\n            ]\n        }\"\n    }\n}\n```\n\nTimePartitionedFileSets are partitioned by year, month, day, hour, and minute. Suppose the\ncurrent run was scheduled to start at 10:00am on January 1, 2015. Since the 'delay'\nproperty is set to one minute, only data before 9:59am January 1, 2015 will be read. This\nexcludes the partition for year 2015, month 1, day 1, hour 9, and minute 59. Since the\n'duration' property is set to five minutes, a total of five partitions will be read. This\nmeans partitions for year 2015, month 1, day 1, hour 9, and minutes 54, 55, 56, 57, and 58\nwill be read. \n\nThe source will read the actual data using the given schema and will output records with\nthis schema:\n\n| field name  | type    |\n| ----------- | ------- |\n| date        | string  |\n| userid      | long    |\n| action      | string  |\n| item        | string  |\n",
    "doc.Table-batchsink": "# Table Batch Sink\n\n\nDescription\n-----------\nWrites records to a CDAP Table with one record field mapping\nto the Table rowkey, and all other record fields mapping to Table columns.\n\n\nUse Case\n--------\nThe sink is used whenever you need to write to a Table in batch. For example,\nyou may want to periodically dump the contents of a relational database into a CDAP Table.\n\n\nProperties\n----------\n**name:** Name of the table dataset. If it does not already exist, one will be created. (Macro-enabled)\n\n**schema:** Optional schema of the table as a JSON Object. If the table does not\nalready exist, one will be created with this schema, which will allow the table to be\nexplored through Hive. Output schema should have columns other than rowkey. (Macro-enabled)\n\n**schema.row.field:** The name of the record field that should be used as the row\nkey when writing to the table. (Macro-enabled)\n\n**case.sensitive.row.field:** Whether 'schema.row.field' is case sensitive; defaults to true.\n\n\nExample\n-------\nThis example writes to a Table named 'users':\n\n```json\n{\n    \"name\": \"Table\",\n    \"type\": \"batchsink\",\n    \"properties\": {\n        \"name\": \"users\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"user\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n            ]\n        }\",\n        \"schema.row.field\": \"id\"\n    }\n}\n```\n\nIt takes records with this schema as input:\n\n| field name     | type                |\n| -------------- | ------------------- |\n| id             | long                |\n| name           | string              |\n| birthyear      | int                 |\n\nThe 'id' field will be used as the rowkey when writing to the table. The 'name' and 'birthyear' record\nfields will be written to columns named 'name' and 'birthyear'.\n",
    "widgets.SnapshotAvro-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"dataset-selector\",\n          \"name\": \"name\",\n          \"label\": \"Dataset Name\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"basePath\",\n          \"label\": \"Snapshot Base Path\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [{\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileProperties\",\n        \"label\": \"FileSet Properties\"\n      }]\n    }\n  ],\n  \"display-name\": \"Avro Snapshot Dataset\"\n}",
    "widgets.TPFSParquet-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Time Partitioned Fileset - Parquet\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Dataset Base Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"filePathFormat\",\n        \"label\": \"Partition Directory Format\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"timeZone\",\n        \"label\": \"Time Zone\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"partitionOffset\",\n        \"label\": \"Partition Offset\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"compressionCodec\",\n        \"label\": \"Compression Codec\",\n        \"widget-attributes\": {\n          \"default\": \"None\",\n          \"values\": [\n            \"None\",\n            \"Snappy\",\n            \"GZip\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Parquet Time Partitioned Dataset\"\n}",
    "doc.Table-batchsource": "# Table Batch Source\n\n\nDescription\n-----------\nReads the entire contents of a CDAP Table. Outputs one record for each row in the Table.\nThe Table must conform to a given schema. \n\n\nUse Case\n--------\nThe source is used whenever you need to read from a table in batch. For example,\nyou may want to periodically dump the contents of a CDAP Table to a relational database.\n\n\nProperties\n----------\n**Name:** Table name. If the table does not already exist, it will be created. (Macro-enabled)\n\n**Row Field:** Optional record field for which row key will be considered as value instead of row column.\nThe field name specified must be present in the schema, and must not be nullable. (Macro-enabled)\n\n**Schema:** Schema of records read from the table. Row columns map to record\nfields. For example, if the schema contains a field named 'user' of type string, the value\nof that field will be taken from the value stored in the 'user' column. Only simple types\nare allowed (boolean, int, long, float, double, bytes, string). (Macro-enabled)\n\n\nExample\n-------\nThis example reads from a Table named 'users':\n\n```json\n{\n    \"name\": \"Table\",\n    \"type\": \"batchsource\",\n    \"properties\": {\n        \"name\": \"users\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"user\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n            ]\n        }\",\n        \"schema.row.field\": \"id\"\n    }\n}\n```\n\nIt outputs records with this schema:\n\n| field name     | type                |\n| -------------- | ------------------- |\n| id             | long                |\n| name           | string              |\n| birthyear      | int                 |\n\nThe 'id' field will be read from the row key of the table. The 'name' field will be read from the\n'name' column in the table. The 'birthyear' field will be read from the 'birthyear' column in the\ntable. Any other columns in the Table will be ignored by the source.\n",
    "widgets.TPFSParquet-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Basic\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Dataset Base Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"duration\",\n        \"label\": \"Duration\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"delay\",\n        \"label\": \"Delay\"\n      }\n    ]\n  }],\n  \"display-name\": \"Parquet Time Partitioned Dataset\"\n}",
    "doc.Email-postaction": "# Email Post-run Action\n\n\nDescription\n-----------\nSends an email at the end of a pipeline run.\n\n\nUse Case\n--------\nThis action can be used when you want to send an email at the end of a pipeline run.\nFor example, you may want to configure a pipeline so that an email is sent whenever\nthe run failed for any reason.\n\n\nProperties\n----------\n**runCondition:**\" When to run the action. Must be 'completion', 'success', or 'failure'. Defaults to 'completion'.\nIf set to 'completion', the action will be executed regardless of whether the pipeline run succeeded or failed.\nIf set to 'success', the action will only be executed if the pipeline run succeeded.\nIf set to 'failure', the action will only be executed if the pipeline run failed.\n\n**recipients:** Comma-separated list of addresses to send the email to.\n\n**sender:** The address to send the email from.\n\n**message:** The message of the email.\n\n**subject:** The subject of the email.\n\n**protocol:** The email protocol to use. SMTP, SMTPS, and TLS are supported. Defaults to SMTP.\n\n**username:** The username to use for authentication if the protocol requires it.\n\n**password:** The password to use for authentication if the protocol requires it.\n\n**host:** The SMTP host to use. Defaults to 'localhost'.\n\n**port:** The SMTP port to use. Defaults to 25.\n\n**includeWorkflowToken:** Whether to include the contents of the workflow token in the email message. Defaults to false.\n\n**configurableJavaMailProperties:** Optional property that can be used to pass specific set of javamail properties.\nThe syntax is '<property_name>:<value>'. If this property is not set, default values are chosen.\n\n\nExample\n-------\nThis example sends an email from 'team-ops@example.com' to 'team-alerts@example.com' whenever a run fails:\n\n    {\n        \"name\": \"Email\",\n        \"type\": \"postaction\",\n        \"properties\": {\n            \"recipientEmailAddress\": \"team-alerts@example.com\",\n            \"senderEmailAddress\": \"team-ops@example.com\",\n            \"subject\": \"Pipeline Failure ${logicalStartTime(yyyy-MM-dd)}\",\n            \"message\": \"The pipeline run failed.\",\n            \"includeWorkflowToken\": \"true\",\n            \"host\": \"smtp-server.com\",\n            \"port\": \"25\",\n            \"runCondition\": \"failure\"\n        }\n    }\n",
    "widgets.FTP-batchsource": "{\n  \"outputs\": [{\n    \"schema\": {\n      \"name\": \"etlSchemaBody\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"offset\",\n          \"type\": \"long\"\n        },\n        {\n          \"name\": \"body\",\n          \"type\": \"string\"\n        }\n      ]\n    },\n    \"widget-type\": \"non-editable-schema-editor\"\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\",\n          \"widget-attributes\": {\"placeholder\": \"Name used to identify this source for lineage\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"path\",\n          \"label\": \"Path\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"fileRegex\",\n          \"label\": \"Regex Path Filter\"\n        },\n        {\n          \"widget-type\": \"json-editor\",\n          \"name\": \"fileSystemProperties\",\n          \"label\": \"File System Properties\"\n        },\n        {\n          \"widget-type\": \"radio-group\",\n          \"name\": \"ignoreNonExistingFolders\",\n          \"label\": \"Allow Empty Input\",\n          \"widget-attributes\": {\n            \"layout\": \"inline\",\n            \"default\": \"false\",\n            \"options\": [\n              {\n                \"id\": \"true\",\n                \"label\": \"True\"\n              },\n              {\n                \"id\": \"false\",\n                \"label\": \"False\"\n              }\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"display-name\": \"FTP (Deprecated)\"\n}",
    "widgets.TMS-alertpublisher": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"topic\",\n        \"label\": \"Topic\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"namespace\",\n        \"label\": \"Namespace\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"autoCreateTopic\",\n        \"label\": \"Auto-Create Topic\",\n        \"widget-attributes\": {\n          \"default\": \"false\",\n          \"values\": [\n            \"false\",\n            \"true\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"maxAlertsPerSecond\",\n        \"label\": \"Rate Limit (alerts per second)\"\n      }\n    ]\n  }],\n  \"display-name\": \"Transactional Alert Publisher\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,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\"},\n    \"type\": \"inline\"\n  }\n}",
    "doc.XMLReader-batchsource": "# XML Reader Batch Source\n\n\nDescription\n-----------\nThe XML Reader plugin is a source plugin that allows users to read XML files stored on HDFS.\n\n\nUse Case\n--------\nA user would like to read XML files that have been dropped into HDFS.\nThese can range in size from small to very large XML files. The XMLReader will read and parse the files,\nand when used in conjunction with the XMLParser plugin, fields can be extracted.\nThis reader emits one XML event, specified by the node path property, for each file read.\n\n\nProperties\n----------\n**Reference Name:** Name used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**Path:** Path to file(s) to be read. If a directory is specified, terminate the path name with a '/'. This leverages glob syntax as described in the [Java Documentation](https://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob).\n\n**Node Path:** Node path (XPath) to emit as an individual event from the XML schema. Example: '/book/price' to read only the price from under the book node. For more information about XPaths, see the [Java Documentation](https://docs.oracle.com/javase/tutorial/jaxp/xslt/xpath.html).\n\n**Action After Processing File:** Action to be taken after processing of the XML file. Possible actions are: (DELETE) delete from HDFS; (ARCHIVE) archive to the target location; and (MOVE) move to the target location.\n\n**Reprocessing Required:** Specifies whether the files should be reprocessed. If set to `No`, the files are tracked and\nwill not be processed again on future runs of the pipeline.\n\n**Temporary Folder:** An existing folder path with read and write access for the current user. This is required for storing temporary files containing paths of the processed XML files. These temporary files will be read at the end of the job to update the file track table. Defaults to `/tmp`.\n\n**File Pattern:** The regular expression pattern used to select specific files. This should be used in cases when the glob syntax in the `Path` is not precise enough. See examples in the Usage Notes.\n\n**Target Folder:** Target folder path if the user select an action for after the process, either one of ARCHIVE or MOVE. Target folder must be an existing directory.\n\n**Table Name:** When keeping track of processed files, this is the name of the Table dataset used to store the data. This is required when reprocessing is set to `No`.\n\n**Table Data Expiry Period (Days):** The amount of time (in days) to wait before clearing the table used to track processed filed. If omitted, data will not expire in the tracking table. Example: for `tableExpiryPeriod = 30`, data before 30 days is deleted from the table.\n\n**Enable external entities:** This enables processing external entities while reading xml file. Defaults to `false`. __Note__: The external entities should be enabled only if necessary. It posts security risk of malicious code execution. Please read more about [xxe xml vulnerability here](https://owasp.org/www-community/vulnerabilities/XML_External_Entity_(XXE)_Processing).\n\n**Support DTDs:** This sets supporting DTDs while processing xml file. This property needs to be set `true` if external entities needs to be evaluated. Defaults to `false`.\n\n\nUsage Notes\n-----------\nWhen specifying a regular expression for filtering files, you must use glob syntax in the folder path.\nThis usually means ending the path with '/*'.\n\nHere are some regular expression pattern examples:\n1. Use '^' to select files with names starting with 'catalog', such as '^catalog'.\n2. Use '$' to select files with names ending with 'catalog.xml', such as 'catalog.xml$'.\n3. Use '.\\*' to select files with a name that contains 'catalogBook', such as 'catalogBook.*'.\n\n\nExample\n-------\nThis example reads data from the folder \"hdfs:/cdap/source/xmls/\" and emits XML records on the basis of the node path\n\"/catalog/book/title\". It will generate structured records with the fields 'offset', 'fileName', and 'record'.\nIt will move the XML files to the target folder \"hdfs:/cdap/target/xmls/\" and update the processed file information\nin the table named \"trackingTable\".\n\n```json\n{\n    \"name\": \"XMLReaderBatchSource\",\n    \"plugin\":{\n      \"name\": \"XMLReaderBatchSource\",\n      \"type\": \"batchsource\",\n      \"properties\":{\n          \"referenceName\": \"referenceName\"\"\n          \"path\": \"hdfs:/cdap/source/xmls/*\",\n          \"Pattern\": \"^catalog.*\"\n          \"nodePath\": \"/catalog/book/title\"\n          \"actionAfterProcess\" : \"Move\",\n          \"targetFolder\":\"hdfs:/cdap/target/xmls/\",\n          \"reprocessingRequired\": \"No\",\n          \"tableName\": \"trackingTable\",\n          \"temporaryFolder\": \"hdfs:/cdap/tmp/\"\n      }\n   }\n}\n```\n\n For this XML as an input:\n\n```xml\n<catalog>\n  <book id=\"bk104\">\n    <author>Corets, Eva<\/author>\n    <title>Oberon's Legacy<\/title>\n    <genre>Fantasy<\/genre>\n    <price><base>5.95<\/base><tax><surcharge>13.00<\/surcharge><excise>13.00<\/excise><\/tax><\/price>\n    <publish_date>2001-03-10<\/publish_date>\n    <description><name><name>In post-apocalypse England, the mysterious\n    agent known only as Oberon helps to create a new life\n    for the inhabitants of London. Sequel to Maeve\n    Ascendant.<\/name><\/name><\/description>\n  <\/book>\n  <book id=\"bk105\">\n    <author>Corets, Eva<\/author>\n    <title>The Sundered Grail<\/title>\n    <genre>Fantasy<\/genre>\n    <price><base>5.95<\/base><tax><surcharge>14.00<\/surcharge><excise>14.00<\/excise><\/tax><\/price>\n    <publish_date>2001-09-10<\/publish_date>\n    <description><name>The two daughters of Maeve, half-sisters,\n    battle one another for control of England. Sequel to\n    Oberon's Legacy.<\/name><\/description>\n  <\/book>\n<\/catalog>\n```\n\n The output records will be:\n\n| offset | filename                            | record                            |\n| ------ | ----------------------------------- | --------------------------------- |\n| 2      | hdfs:/cdap/source/xmls/catalog.xml  | <title>Oberon's Legacy<\/title>    |\n| 13     | hdfs:/cdap/source/xmls/catalog.xml  | <title>The Sundered Grail<\/title> |\n",
    "widgets.Cube-batchsink": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"General\",\n      \"properties\": [{\n        \"widget-type\": \"textbox\",\n        \"name\": \"name\",\n        \"label\": \"Name\"\n      }]\n    },\n    {\n      \"label\": \"Cube Dataset Properties (if creating new dataset)\",\n      \"properties\": [\n        {\n          \"widget-type\": \"csv\",\n          \"name\": \"dataset.cube.resolutions\",\n          \"label\": \"Cube Resolution\",\n          \"widget-attributes\": {\"delimiter\": \",\"}\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"dataset.cube.aggregations\",\n          \"label\": \"Aggregations\",\n          \"widget-attributes\": {\n            \"kv-delimiter\": \":\",\n            \"delimiter\": \";\",\n            \"showDelimiter\": \"false\"\n          }\n        },\n        {\n          \"widget-type\": \"json-editor\",\n          \"name\": \"dataset.cube.properties\",\n          \"label\": \"Other Dataset Properties\"\n        }\n      ]\n    },\n    {\n      \"label\": \"CubeFact Mapping\",\n      \"properties\": [\n        {\n          \"widget-type\": \"keyvalue-dropdown\",\n          \"name\": \"cubeFact.measurements\",\n          \"label\": \"Measurements\",\n          \"widget-attributes\": {\n            \"kv-delimiter\": \":\",\n            \"dropdownOptions\": [\n              \"COUNTER\",\n              \"GAUGE\"\n            ],\n            \"delimiter\": \";\",\n            \"showDelimiter\": \"false\"\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"cubeFact.timestamp.field\",\n          \"label\": \"Timestamp Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"cubeFact.timestamp.format\",\n          \"label\": \"Timestamp Format\"\n        }\n      ]\n    }\n  ]\n}",
    "doc.FileDelete-action": "# File Delete Action\n\n\nDescription\n-----------\nDeletes a file or files.\n\n\nUse Case\n--------\nThis action can be used to remove a file or files.\n\n\nProperties\n----------\n**path:** The full path of the file or files that need to be deleted. If the path points to a file,\nthe file will be removed. If the path points to a directory with no regex specified, the directory and all of \nits contents will be removed. If a regex is specified, only the files and directories matching that regex\nwill be removed.\n\n**fileRegex:** Wildcard regular expression to filter the files in the source directory that will be removed.\n\n**continueOnError:** Indicates if the pipeline should continue if the delete process fails. If all files are not \nsuccessfully deleted, the action will not re-create the files already deleted.\n\n\nExample\n-------\nThis example deletes all files ending in `.txt` from `/source/path`:\n\n    {\n        \"name\": \"FileDelete\",\n        \"plugin\": {\n            \"name\": \"FileDelete\",\n            \"type\": \"action\",\n            \"artifact\": {\n                \"name\": \"core-plugins\",\n                \"version\": \"1.4.0-SNAPSHOT\",\n                \"scope\": \"SYSTEM\"\n            },\n            \"properties\": {\n                \"path\": \"hdfs://example.com:8020/source/path\",\n                \"fileRegex\": \".*\\.txt\",\n                \"continueOnError\": \"false\"\n            }\n        }\n    }\n",
    "doc.TPFSParquet-batchsink": "# TimePartitionedFileSet Parquet Batch Sink\n\n\nDescription\n-----------\nSink for a ``TimePartitionedFileSet`` that writes data in Parquet format.\nEvery time the pipeline runs, a new partition in the ``TimePartitionedFileSet``\nwill be created based on the logical start time of the run.\nAll data for the run will be written to that partition.\n\n\nUse Case\n--------\nThis sink is used whenever you want to write to a ``TimePartitionedFileSet`` in Parquet format.\nFor example, you might want to create daily snapshots of a database table by reading\nthe entire contents of the table and writing to this sink.\n\n\nProperties\n----------\n**name:** Name of the ``TimePartitionedFileSet`` to which records are written.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**schema:** The Avro schema of the record being written to the sink as a JSON Object. (Macro-enabled)\n\n**basePath:** Base path for the ``TimePartitionedFileSet``. Defaults to the name of the dataset. (Macro-enabled)\n\n**filePathFormat:** Format for the time partition, as used by ``SimpleDateFormat``.\nDefaults to formatting partitions such as ``2015-01-01/20-42.142017372000``. (Macro-enabled)\n\n**timeZone:** The string ID for the time zone to format the date in. Defaults to using UTC.\nThis setting is only used if ``filePathFormat`` is not null. (Macro-enabled)\n\n**partitionOffset:** Amount of time to subtract from the pipeline runtime to determine the output partition. Defaults to 0m.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit,\nwith 's' for seconds, 'm' for minutes, 'h' for hours, and 'd' for days.\nFor example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand the offset is set to '1d', data will be written to the partition for midnight Dec 31, 2015.\" (Macro-enabled)\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, GZip.\n\nExample\n-------\nThis example will write to a ``TimePartitionedFileSet`` named ``'users'``:\n\n    {\n        \"name\": \"TPFSParquet\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"filePathFormat\": \"yyyy-MM-dd/HH-mm\",\n            \"timeZone\": \"America/Los_Angeles\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }\n\nIt will write data in Parquet format using the given schema. Every time the pipeline runs, a\nnew partition in the ``TimePartitionedFileSet`` will be created based on the logical start\ntime of the run with the output directory ending with the date formatted as specified. All\ndata for the run will be written to that partition and compressed using the Snappy codec.\n\nFor example, if the pipeline was scheduled to run at 10:00am on January 1, 2015 in Los\nAngeles, a new partition will be created with year 2015, month 1, day 1, hour 10, and\nminute 0, and the output directory for that partition would end with ``2015-01-01/10-00``.",
    "doc.FTP-batchsource": "# FTP Batch Source (Deprecated)\n\n**NOTE**: This plugin is deprecated. Please use https://github.com/data-integrations/ftp-plugins instead, which is\navailable as FTP plugins (> v3.0.0) from the Hub.\n\nDescription\n-----------\nBatch source for an FTP or SFTP source. Prefix of the path ('ftp://...' or 'sftp://...') determines the source server\ntype, either FTP or SFTP.\n\n\nUse Case\n--------\nThis source is used whenever you need to read from an FTP or SFTP server.\n\n\nProperties\n----------\n**Reference Name:** Name used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**Path:** Path to file(s) to be read. The path uses filename expansion (globbing) to read files.\nPath is expected to be of the form prefix://username:password@hostname:port/path (Macro-enabled)\n\n**Regex Path Filter:** Regex to filter out files in the path. It accepts regular expression which is applied to the complete\npath and returns the list of files that match the specified pattern. (Macro-enabled)\n\n**File System Properties:** A JSON string representing a map of properties\nneeded for the distributed file system. (Macro-enabled)\n\n**Allow Empty Input:** Identify if path needs to be ignored or not, for case when directory or file does not\nexists. If set to true it will treat the not present folder as 0 input and log a warning. Default is false.\n\n\nExample\n-------\nThis example connects to an SFTP server and reads in files found in the specified directory.\n\n    {\n        \"name\": \"FTP\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"path\": \"sftp://username:password@hostname:21/path/to/logs\",\n            \"ignoreNonExistingFolders\": \"false\",\n            \"recursive\": \"false\"\n        }\n    }\n",
    "widgets.Joiner-batchjoiner": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\"\n  }],\n  \"metadata\": {\"spec-version\": \"1.3\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"sql-select-fields\",\n          \"name\": \"selectedFields\",\n          \"description\": \"List of fields to be selected and/or renamed in the Joiner output from each stages. There must not be a duplicate fields in the output.\",\n          \"label\": \"Fields\"\n        },\n        {\n          \"widget-type\": \"join-types\",\n          \"name\": \"requiredInputs\",\n          \"description\": \"Type of joins to be performed. Inner join means all stages are required, while Outer join allows for 0 or more input stages to be required input.\",\n          \"label\": \"Join Type\"\n        },\n        {\n          \"widget-type\": \"radio-group\",\n          \"name\": \"conditionType\",\n          \"label\": \"Join Condition Type\",\n          \"widget-attributes\": {\n            \"layout\": \"inline\",\n            \"default\": \"basic\",\n            \"options\": [\n              {\n                \"label\": \"Basic\",\n                \"id\": \"basic\"\n              },\n              {\n                \"label\": \"Advanced\",\n                \"id\": \"advanced\"\n              }\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"sql-conditions\",\n          \"name\": \"joinKeys\",\n          \"description\": \"List of join keys to perform join operation.\",\n          \"label\": \"Join Condition\"\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"inputAliases\",\n          \"label\": \"Input Aliases\",\n          \"widget-attributes\": {\n            \"kv-delimiter\": \"=\",\n            \"key-placeholder\": \"Input Name\",\n            \"value-placeholder\": \"Alias\",\n            \"delimiter\": \";\",\n            \"showDelimiter\": \"false\"\n          }\n        },\n        {\n          \"widget-type\": \"textarea\",\n          \"name\": \"conditionExpression\",\n          \"label\": \"Join Condition\"\n        },\n        {\n          \"widget-type\": \"get-schema\",\n          \"widget-category\": \"plugin\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [\n        {\n          \"widget-type\": \"multiple-input-stage-selector\",\n          \"name\": \"inMemoryInputs\",\n          \"label\": \"Inputs to Load in Memory\"\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"joinNullKeys\",\n          \"label\": \"Join on Null Keys\",\n          \"widget-attributes\": {\n            \"default\": \"true\",\n            \"off\": {\n              \"label\": \"False\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"True\",\n              \"value\": \"true\"\n            }\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"numPartitions\",\n          \"label\": \"Number of Partitions\"\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"distributionEnabled\",\n          \"label\": \"Distribution Enabled\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"off\": {\n              \"label\": \"False\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"True\",\n              \"value\": \"true\"\n            }\n          }\n        },\n        {\n          \"name\": \"distributionFactor\",\n          \"widget-type\": \"number\",\n          \"label\": \"Distribution Size\",\n          \"widget-attributes\": {\"min\": \"0\"}\n        },\n        {\n          \"widget-type\": \"multiple-input-stage-selector\",\n          \"name\": \"distributionStageName\",\n          \"label\": \"Distribution Skewed Input Stage\",\n          \"widget-attributes\": {\"singleSelectOnly\": \"true\"}\n        }\n      ]\n    }\n  ],\n  \"inputs\": {\"multipleInputs\": true},\n  \"filters\": [\n    {\n      \"condition\": {\n        \"property\": \"distributionEnabled\",\n        \"value\": \"true\",\n        \"operator\": \"equal to\"\n      },\n      \"name\": \"distribution rules\",\n      \"show\": [\n        {\"name\": \"distributionFactor\"},\n        {\"name\": \"distributionStageName\"}\n      ]\n    },\n    {\n      \"condition\": {\"expression\": \"conditionType == 'basic' || !conditionType\"},\n      \"name\": \"basic condition\",\n      \"show\": [\n        {\"name\": \"distributionEnabled\"},\n        {\"name\": \"joinKeys\"},\n        {\"name\": \"numPartitions\"},\n        {\"name\": \"joinNullKeys\"}\n      ]\n    },\n    {\n      \"condition\": {\n        \"property\": \"conditionType\",\n        \"value\": \"advanced\",\n        \"operator\": \"equal to\"\n      },\n      \"name\": \"advanced condition\",\n      \"show\": [\n        {\"name\": \"conditionExpression\"},\n        {\"name\": \"inputAliases\"}\n      ]\n    }\n  ]\n}",
    "widgets.LogParser-transform": "{\n  \"outputs\": [{\n    \"schema\": {\n      \"name\": \"etlSchemaBody\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"uri\",\n          \"type\": \"string\"\n        },\n        {\n          \"name\": \"ip\",\n          \"type\": \"string\"\n        },\n        {\n          \"name\": \"browser\",\n          \"type\": \"string\"\n        },\n        {\n          \"name\": \"device\",\n          \"type\": \"string\"\n        },\n        {\n          \"name\": \"httpStatus\",\n          \"type\": \"int\"\n        },\n        {\n          \"name\": \"ts\",\n          \"type\": \"long\"\n        }\n      ]\n    },\n    \"widget-type\": \"non-editable-schema-editor\"\n  }],\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Log Parser Transform\",\n    \"properties\": [\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"logFormat\",\n        \"label\": \"Log Format\",\n        \"widget-attributes\": {\n          \"default\": \"CLF\",\n          \"values\": [\n            \"CLF\",\n            \"S3\",\n            \"Cloudfront\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"inputName\",\n        \"label\": \"Input Name\"\n      }\n    ]\n  }],\n  \"errorDataset\": {\"errorDatasetTooltip\": \"Dataset that collects error messages from emitter.\"},\n  \"display-name\": \"Webserver Log Parser\"\n}",
    "widgets.SSH-action": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"SSH Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"host\",\n        \"label\": \"Hostname\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"port\",\n        \"label\": \"Port\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"user\",\n        \"label\": \"Username\"\n      },\n      {\n        \"widget-type\": \"textarea\",\n        \"name\": \"privateKey\",\n        \"label\": \"Private Key\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"passphrase\",\n        \"label\": \"Private Key Passphrase\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"command\",\n        \"label\": \"Command\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"outputKey\",\n        \"label\": \"Key Name for Script Output\"\n      }\n    ]\n  }],\n  \"display-name\": \"Remote Program Executor\"\n}",
    "doc.JavaScript-transform": "# JavaScript Transform\n\n\nDescription\n-----------\nExecutes user-provided JavaScript that transforms one record into zero or more records.\nInput records are converted into JSON objects which can be directly accessed in\nJavaScript. The transform expects to receive a JSON object as input, which it can\nprocess and emit zero or more records or emit error using the provided emitter object.\nCurrently, JDK 8's Nashorn JavaScript engine is used to run the user's code which\nsupports ES5 syntax.\n\n\nUse Case\n--------\nThe JavaScript transform is used when other transforms cannot meet your needs.\nFor example, you may want to multiply a field by 1024 and rename it from ``'gigabytes'``\nto ``'megabytes'``. Or you might want to convert a timestamp into a human-readable date string.\n\n\nProperties\n----------\n**script:** JavaScript defining how to transform input record into zero or more records. The script must\nimplement a function called ``'transform'``, which takes as input a JSON object (representing\nthe input record), an emitter object (to emit zero or more output records), \nand a context object (which encapsulates CDAP metrics, logger, arguments, and lookup).\nArguments contain any preferences stored for the pipeline, overridden by runtime arguments for the pipeline\nrun, overridden by any arguments set by stages earlier in the pipeline.\nConsider the following example:\n\n```js\nfunction transform(input, emitter, context) {\n    if (context.getLookup('blacklist').lookup(input.id) != null) {\n        emitter.emitError({\n            'errorCode': 31,\n            'errorMsg': 'blacklisted id',\n            'invalidRecord': input\n        });\n        return;\n    }\n\n    var threshold = context.getArguments().get('priceThreshold');\n    if (input.price > threshold) {\n        emitter.emitAlert({\n            'price': '' + input.price\n        });\n        return;\n    }\n\n    if (input.count < 0) {\n        context.getMetrics().count('negative.count', 1);\n        context.getLogger().debug('Received record with negative count');\n    }\n    input.count = input.count * 1024;\n    emitter.emit(input);\n}\n```\n\nThis script will emit an error if the ``id`` field is present in blacklist table, read a price threshold from\nthe pipeline arguments and emit an alert if the input price is greater than the threshold,\nor else scale the ``count`` field by 1024.\n\n**schema:** The schema of output objects. If no schema is given, it is assumed that the output\nschema is the same as the input schema.\n\n**lookup:** The configuration of the lookup tables to be used in your script.\nFor example, if lookup table \"purchases\" is configured, then you will be able to perform\noperations with that lookup table in your script: ``context.getLookup('purchases').lookup('key')``\nCurrently supports ``KeyValueTable``.\n\n\nExample\n-------\n\n```js\n{\n    \"name\": \"JavaScript\",\n    \"type\": \"transform\",\n    \"properties\": {\n        \"script\": \"function transform(input, emitter, context) {\n                var tax = input.subtotal * 0.0975;\n                if (tax > 1000.0) {\n                    context.getMetrics().count(\"tax.above.1000\", 1);\n                }\n                if (tax < 0.0) {\n                    context.getLogger().info(\"Received record with negative subtotal\");\n                }\n                emitter.emit( {\n                    'subtotal': input.subtotal,\n                    'tax': tax,\n                    'total': input.subtotal + tax\n                });\n                }\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"expanded\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"subtotal\\\",\\\"type\\\":\\\"double\\\"},\n                {\\\"name\\\":\\\"tax\\\",\\\"type\\\":\\\"double\\\"},\n                {\\\"name\\\":\\\"total\\\",\\\"type\\\":\\\"double\\\"}\n            ]\n        }\",\n        \"lookup\": \"{\n            \\\"tables\\\":{\n                \\\"purchases\\\":{\n                    \\\"type\\\":\\\"DATASET\\\",\n                    \\\"datasetProperties\\\":{\n                        \\\"dataset_argument1\\\":\\\"foo\\\",\n                        \\\"dataset_argument2\\\":\\\"bar\\\"\n                    }\n                }\n            }\n        }\"\n    }\n}\n```\n\nThe transform takes records that have a ``'subtotal'`` field, calculates ``'tax'`` and\n``'total'`` fields based on the subtotal, and then returns a record containing those three\nfields. For example, if it receives as an input record:\n\n| field name | type                | value                |\n| ---------- | ------------------- | -------------------- |\n| subtotal   | double              | 100.0                |\n| user       | string              | \"samuel\"             |\n\nit will transform it to this output record:\n\n| field name | type                | value                |\n| ---------- | ------------------- | -------------------- |\n| subtotal   | double              | 100.0                |\n| tax        | double              | 9.75                 |\n| total      | double              | 109.75               |\n",
    "doc.SnapshotAvro-batchsource": "# Snapshot Avro Batch Source\n\n\nDescription\n-----------\nA batch source that reads from a corresponding SnapshotAvro sink.\nThe source will only read the most recent snapshot written to the sink.\n\n\nUse Case\n--------\nThis source is used whenever you want to read data written to the corresponding\nSnapshotAvro sink. It will read only the last snapshot written to that sink. For example,\nyou might want to create daily snapshots of a database by reading the entire contents of a\ntable and writing it to a SnapshotAvro sink. You might then want to use this source to\nread the most recent snapshot and run some data analysis on it.\n\n\nProperties\n----------\n**Dataset Name:** Name of the PartitionedFileSet to read from.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**Snapshot Base Path:** Base path for the PartitionedFileSet. Defaults to the name of the dataset. (Macro-enabled)\n\n**FileSet Properties:** Advanced feature to specify any additional properties that should be used with the source,\nspecified as a JSON object of string to string. These properties are set on the dataset if one is created.\nThe properties are also passed to the dataset at runtime as arguments. (Macro-enabled)\n\n**Schema:** The Avro schema of the record being written to the sink as a JSON object.\n\n\nExample\n-------\nThis example will read from a SnapshotFileSet named 'users'. It will read data in Avro format\nusing the given schema. Every time the pipeline runs, only the most recently added snapshot will\nbe read:\n\n    {\n        \"name\": \"SnapshotAvro\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }\n",
    "doc.TMS-alertpublisher": "# TMS Alert Publisher\n\n\nDescription\n-----------\nPublishes alerts to the CDAP Transactional Messaging System (TMS) as json objects. The plugin\nallows you to specify the topic and namespace to publish to, as well as a rate limit for the\nmaximum number of alerts to publish per second.\n\n\nProperties\n----------\n**topic:** The topic to publish alerts to. (Macro-enabled)\n\n**namespace:** The namespace of the topic to publish alerts to.\nIf none is specified, defaults to the namespace of the pipeline. (Macro-enabled)\n\n**autoCreateTopic:** Whether to create the topic in the pipeline namespace if the topic does not already exist.\nCannot be set to true if namespace is set. Defaults to false.\n\n**maxAlertsPerSecond:** The maximum number of alerts to publish per second. Defaults to 100.\n",
    "doc.KVTable-batchsink": "# KeyValueTable Batch Sink\n\n\nDescription\n-----------\nWrites records to a KeyValueTable, using configurable fields from input records as the\nkey and value.\n\nUse Case\n--------\nThe source is used whenever you need to write to a KeyValueTable in batch. For example,\nyou may want to periodically copy portions of a Table into a KeyValueTable.\n\nProperties\n----------\n**name:** Name of the dataset. If it does not already exist, one will be created.\n\n**key.field:** The name of the field to use as the key. Defaults to 'key'.\n\n**value.field:** The name of the field to use as the value. Defaults to 'value'.\n\nExample\n-------\nThis example writes to a KeyValueTable named 'items':\n\n    {\n        \"name\": \"KVTable\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"items\",\n            \"key.field\": \"id\",\n            \"value.field\": \"description\"\n        }\n    }\n\nIt takes records with the following schema as input:\n\n| field name     | type                |\n| -------------- | ------------------- |\n| id             | bytes               |\n| description    | bytes               |\n\nWhen writing to the KeyValueTable, the 'id' field will be used as the key,\nand the 'description' field will be used as the value.\n",
    "widgets.FileMove-action": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"sourcePath\",\n        \"label\": \"Source Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"destPath\",\n        \"label\": \"Destination Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"fileRegex\",\n        \"label\": \"Wildcard\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"continueOnError\",\n        \"label\": \"Continue Pipeline Upon Error\",\n        \"widget-attributes\": {\n          \"default\": \"false\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAA3CAYAAACYV8NnAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJ\\r\\nbWFnZVJlYWR5ccllPAAAAcBJREFUeNrs2rttwzAQAFCKYZHStSpnAxlqUtobeASrTOkNkgmCTBBv\\r\\nkBHkBQRwBHcqo0yQHIGLISCQxI9M3gU6gJbd0A/E+Xw6OxMeUZblAS7vTdNk+PpbxIkO1lH6gkWa\\r\\nWJn3lozA11AOYIM9CAIhuYGt0NTAk2iK4FE0VfAgmjL4T/UArKmDNaxCEA7JDXxFcwL3T5oNuJ/T\\r\\nLx7oLhU6C90AU+sz+knDG9eBnVeS9NgKRiEFw1jQA6E5oo+wKnbpATfAJ7js5qrt0XIa4Oe54FE/\\r\\niAA3+b0JzfPo1QPgFzxxzQaN8A6WOfHTrUYI50BjN4KvoIUQrndJk2Ot39HXLcN1CETiGxFLYsUK\\r\\n3YNvbEoiqd4DS+JkLSfXMCH8YawkkuzyTEkcO3GyrSnCL6zQWAYLNuipuq0IgifniIoQ1tzVv9p8\\r\\npStCYOspl7LYMHS8oLESDO2/hsuHcJhw2Zx0HYjeDXWKAC5wf6eBj0yYElsfcDI0ljQvcBI0gJ9F\\r\\n4A+oKjJ4lt9yVCTsCk93z2Xuscb83c+1YYyTnv0PAMvUdEH/V7TmiH7jhL4zD23b6jzPv+DpI6x7\\r\\nwl7T4j79CDAAJUWAH4KPYogAAAAASUVORK5C\"},\n    \"type\": \"inline\"\n  }\n}",
    "doc.RowDenormalizer-batchaggregator": "# Row Denormalizer\n\n\nDescription\n-----------\nConverts raw data into denormalized data based on a key column. User is able to specify the list of fields that should be used in the denormalized record, with an option to use an alias for the output field name. For example, 'ADDRESS' in the input is mapped to 'addr' in the output schema. \n\nUse Case\n--------\nThe transform takes input record that stores a variable set of custom attributes for an entity, denormalizes it on the basis of the key field, and then returns a denormalized table according to the output schema specified by the user.\nThe denormalized data is easier to query.\n\nProperties\n----------\n**keyField:** Name of the column in the input record which will be used to group the raw data. For Example, id.\n\n**nameField:** Name of the column in the input record which contains the names of output schema columns. For example,\n input records have columns 'id', 'attribute', 'value' and the 'attribute' column contains 'FirstName', 'LastName',\n 'Address'.\n  \"So the output record will have column names as 'FirstName', 'LastName', 'Address'.\n\n**valueField:** Name of the column in the input record which contains the values for output schema columns. For\nexample, input records have columns 'id', 'attribute', 'value' and the 'value' column contains 'John',\n'Wagh', 'NE Lakeside'. So the output record will have values for columns 'FirstName', 'LastName', 'Address' as 'John', 'Wagh', 'NE Lakeside' respectively.\n\n**outputFields:** List of the output fields to be included in denormalized output.\n\n**fieldAliases:** List of the output fields to rename. The key specifies the name of the field to rename, with its corresponding value specifying the new name for that field.\n\n**numPartitions:** Number of partitions to use when grouping data. If not specified, the execution framework will\ndecide on the number to use.\n\nConditions\n----------\nIn case a field value is not present, then it will be considered as NULL.\n\nFor Example,\n\nIf keyfield('id') in the input record is NULL, then that particular record will be filtered out.\n\nIf namefield('attribute') or valuefield('value') is not present for a particular keyfield('id') value, then the\ndenormalized output value for that namefield will be NULL.\n\nIf user provides output field which is not present in the input record, then it will be considered as NULL.\n\nExample\n-------\nThe transform takes input records that have columns id, attribute, value, denormalizes it on the basis of\nid, and then returns a denormalized table according to the output schema specified by the user.\n\n```json\n{\n    \"name\": \"RowDenormalizer\",\n    \"type\": \"batchaggregator\",\n    \"properties\": {\n        \"outputFields\": \"Firstname,Lastname,Address\",\n        \"fieldAliases\": \"Address:Office Address\",\n        \"keyField\": \"id\",\n        \"nameField\": \"attribute\",\n        \"valueField\": \"value\"\n    }\n}\n```\nFor example, suppose the aggregator receives the input record:\n\n| id        | attribute   | value      |\n| --------- | ----------- | ---------- |\n| joltie    | Firstname   | John       |\n| joltie    | Lastname    | Wagh       |\n| joltie    | Address     | NE Lakeside|\n\nOutput records will contain all the output fields specified by user:\n\n| id        | Firstname   | Lastname   |  Office Address  |\n| --------- | ----------- | ---------- | ---------------- |\n| joltie    | John        | Wagh       |  NE Lakeside     |\n\n\nNow, let's suppose the aggregator receives the input record with NULL values:\n\n| id        | attribute   | value      |\n| --------- | ----------- | ---------- |\n| joltie    | Firstname   | John       |\n| joltie    | Lastname    | Wagh       |\n| joltie    | Address     | NE Lakeside|\n| brett     | Firstname   | Brett      |\n|           | Lastname    | Lee        |\n| brett     | Address     | SE Lakeside|\n| bob       | Firstname   | Bob        |\n| bob       |             | Smith      |\n| bob       | Address     |            |\n\nOutput records will contain all the output fields specified by user:\n\n| id        | Firstname   | Lastname   |  Office Address  |\n| --------- | ----------- | ---------- | ---------------- |\n| joltie    | John        | Wagh       |  NE Lakeside     |\n| brett     | Brett       |            |  SE Lakeside     |\n| bob       | Bob         |            |                  |\n",
    "widgets.File-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {}\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\",\n          \"widget-attributes\": {\"placeholder\": \"Name used to identify this sink for lineage\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"path\",\n          \"label\": \"Path\",\n          \"widget-attributes\": {\"placeholder\": \"/path/to/output\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"suffix\",\n          \"label\": \"Path Suffix\",\n          \"widget-attributes\": {\"default\": \"yyyy-MM-dd-HH-mm\"}\n        },\n        {\n          \"widget-type\": \"plugin-list\",\n          \"name\": \"format\",\n          \"label\": \"Format\",\n          \"widget-attributes\": {\"plugin-type\": \"validatingOutputFormat\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"delimiter\",\n          \"label\": \"Delimiter\",\n          \"widget-attributes\": {\"placeholder\": \"Delimiter if the format is 'delimited'\"}\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"writeHeader\",\n          \"label\": \"Write Header\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"off\": {\n              \"label\": \"False\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"True\",\n              \"value\": \"true\"\n            }\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [{\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileSystemProperties\",\n        \"label\": \"File System Properties\"\n      }]\n    }\n  ],\n  \"filters\": [\n    {\n      \"condition\": {\n        \"property\": \"format\",\n        \"value\": \"delimited\",\n        \"operator\": \"equal to\"\n      },\n      \"name\": \"delimiter\",\n      \"show\": [{\"name\": \"delimiter\"}]\n    },\n    {\n      \"condition\": {\"expression\": \"format == 'delimited' || format == 'csv' || format == 'tsv'\"},\n      \"name\": \"header\",\n      \"show\": [{\"name\": \"writeHeader\"}]\n    }\n  ]\n}",
    "doc.TPFSAvro-batchsource": "# TimePartitionedFileSet Avro Batch Source\n\n\nDescription\n-----------\nReads from a TimePartitionedFileSet whose data is in Avro format.\n\n\nUse Case\n--------\nThe source is used when you need to read partitions of a TimePartitionedFileSet.\nFor example, suppose there is an application that ingests data by writing to a TimePartitionedFileSet,\nwhere arrival time of the data is used as the partition key. You may want to create a pipeline that\nreads the newly-arrived files, performs data validation and cleansing, and then writes to a Table.\n\n\nProperties\n----------\n**Dataset Name:** Name of the TimePartitionedFileSet from which the records are to be read from. (Macro-enabled)\n\n**Dataset Base Path:** Base path for the TimePartitionedFileSet. Defaults to the name of the\ndataset. (Macro-enabled)\n\n**Duration:** Size of the time window to read with each run of the pipeline. The format is\nexpected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with\n's' for seconds, 'm' for minutes, 'h' for hours, and 'd' for days. For example, a value of\n'5m' means each run of the pipeline will read 5 minutes of events from the TPFS source. (Macro-enabled)\n\n**Delay:** Optional delay for reading from TPFS source. The value must be of the same\nformat as the duration value. For example, a duration of '5m' and a delay of '10m' means\neach run of the pipeline will read events 5 minutes of data from 15 minutes before its logical\nstart time to 10 minutes before its logical start time. The default value is 0. (Macro-enabled)\n\n**Schema:** The Avro schema of the record being read from the source as a JSON Object.\n\n\nExample\n-------\nThis example reads from a TimePartitionedFileSet named 'webactivity', assuming the underlying\nfiles are in Avro format:\n\n```json\n{\n    \"name\": \"TPFSAvro\",\n    \"type\": \"batchsource\",\n    \"properties\": {\n        \"name\": \"webactivity\",\n        \"duration\": \"5m\",\n        \"delay\": \"1m\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"webactivity\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"userid\\\",\\\"type\\\":\\\"long\\\"},\n                {\\\"name\\\":\\\"action\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"}\n            ]\n        }\"\n    }\n}\n```\n\nTimePartitionedFileSets are partitioned by year, month, day, hour, and minute. Suppose the\ncurrent run was scheduled to start at 10:00am on January 1, 2015. Since the 'delay'\nproperty is set to one minute, only data before 9:59am January 1, 2015 will be read. This\nexcludes the partition for year 2015, month 1, day 1, hour 9, and minute 59. Since the\n'duration' property is set to five minutes, a total of five partitions will be read. This\nmeans partitions for year 2015, month 1, day 1, hour 9, and minutes 54, 55, 56, 57, and 58\nwill be read. \n\nThe source will read the actual data using the given schema and will output records with\nthis schema:\n\n| field name  | type    |\n| ----------- | ------- |\n| date        | string  |\n| userid      | long    |\n| action      | string  |\n| item        | string  |\n",
    "doc.Distinct-batchaggregator": "# Distinct Aggregator\n\nDescription\n-----------\nDe-duplicates input records so that all output records are distinct.\nCan optionally take a list of fields, which will project out all other fields and perform a distinct on just those fields.\n\n\nUse Case\n--------\nThis plugin is used when you want to ensure that all output records are unique.\n\n\nProperties\n----------\n**fields:** Optional comma-separated list of fields to perform the distinct on. If not given, all fields are used.\n\n**numPartitions:** Number of partitions to use when grouping fields. If not specified, the execution\nframework will decide on the number to use.\n\nExample\n-------\n```json\n    {\n        \"name\": \"Distinct\",\n        \"type\": \"batchaggregator\"\n        \"properties\": {\n            \"fields\": \"user,item,action\"\n        }\n    }\n```\n\nThis example takes the ``user``, ``action``, and ``item`` fields from input records and dedupes them so that every\noutput record is a unique record with those three fields. For example, if the input to the plugin is:\n\n\n| user  | item   | action | timestamp |\n| ----- | ------ | ------ | --------- |\n| bob   | donut  | buy    | 1000      |\n| bob   | donut  | buy    | 1000      |\n| bob   | donut  | buy    | 1001      |\n| bob   | coffee | buy    | 1001      |\n| bob   | coffee | drink  | 1010      |\n| bob   | donut  | eat    | 1050      |\n| bob   | donut  | eat    | 1080      |\n\nthen records output will be:\n\n\n| user  | item   | action |\n| ----- | ------ | ------ |\n| bob   | donut  | buy    |\n| bob   | coffee | buy    |\n| bob   | coffee | drink  |\n| bob   | donut  | eat    |\n",
    "widgets.Email-postaction": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Email Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"runCondition\",\n        \"label\": \"Run Condition\",\n        \"widget-attributes\": {\n          \"default\": \"completion\",\n          \"values\": [\n            \"completion\",\n            \"success\",\n            \"failure\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"sender\",\n        \"label\": \"Sender\"\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"recipients\",\n        \"label\": \"Recipients\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Email\",\n          \"delimiter\": \",\"\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"subject\",\n        \"label\": \"Subject\"\n      },\n      {\n        \"widget-type\": \"textarea\",\n        \"name\": \"message\",\n        \"label\": \"Message\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"username\",\n        \"label\": \"Username\"\n      },\n      {\n        \"widget-type\": \"password\",\n        \"name\": \"password\",\n        \"label\": \"Password\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"protocol\",\n        \"label\": \"Protocol\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"host\",\n        \"label\": \"Host\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"port\",\n        \"label\": \"Port\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"includeWorkflowToken\",\n        \"label\": \"Include Workflow Token\",\n        \"widget-attributes\": {\n          \"default\": \"false\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"keyvalue\",\n        \"name\": \"configurableJavaMailProperties\",\n        \"label\": \"Configurable Java Mail Properties\",\n        \"widget-attributes\": {\n          \"kv-delimiter\": \"=\",\n          \"delimiter\": \",\",\n          \"placeholder\": \"Java Mail Properties.\"\n        }\n      }\n    ]\n  }]\n}",
    "widgets.Table-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"General\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Name\"\n      },\n      {\n        \"widget-type\": \"input-field-selector\",\n        \"name\": \"schema.row.field\",\n        \"label\": \"Row Field\"\n      }\n    ]\n  }],\n  \"display-name\": \"CDAP Table Dataset\"\n}",
    "widgets.FileDelete-action": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"path\",\n        \"label\": \"Path to desired file(s) to be removed\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"fileRegex\",\n        \"label\": \"Wildcard\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"continueOnError\",\n        \"label\": \"Continue Pipeline Upon Error\",\n        \"widget-attributes\": {\n          \"default\": \"false\",\n          \"values\": [\n            \"true\",\n            \"false\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAA3CAYAAACYV8NnAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJ\\r\\nbWFnZVJlYWR5ccllPAAAAZFJREFUeNrsmEFuwjAQRR23iy67zoYeoVI2XcIJ4AZlyw5O0HKCihPQ\\r\\nniC9AXCASO4N6CpL6AnaGWlAkau2YYzisTpfGqIgJ3lyJvafyQxDRVGM4bCsqiqj80/TjfYQM8sF\\r\\nNnF0jc+2CQEfdXkCMMKOjQDZ1IBbQUsD/hNaIvCv0FKBf4SWDPxt9QBYXAdXELdGsGxqwEfolICb\\r\\nM50McDOn5wzofSzoLPQGlFq7zmcaHrwKdF5R0qNvEpI1CUqhxRUBJ2gOteOjt8KM4FCedcnrsDCN\\r\\nPtO4uTxDfDCv73GdZAj0ANLABW5MG06hzP0QX0OBUXAPfFNOc7qFcKbeA65/iAH9Aq94HZDTLOiz\\r\\nbC7YeYIoyfEd/nvC8MbhmKmUHfEeYuR58ilFUzhmqN5DoRVaoRVaoRVaoRVaeBFwsKILiK1XOs2o\\r\\nePWLhjeyqezWcmi5tYYiYMD04CVZ1Sh9D9eY6Ta6If/dj5Uehoz/Uj9EhVbofwrtUoRepAR9gT91\\r\\nXbs8z7HPfAdxJZgXe+KTLwEGABx9YYJAxeFRAAAAAElFTkSuQmCC\"},\n    \"type\": \"inline\"\n  }\n}",
    "doc.File-connector": "# File Connector\n\n\nDescription\n-----------\nFile connector can be used to browse and sample the local file system\n\nPath of the connection\n----------------------\nTo browse, get a sample from, or get the specification for this connection through\n[Pipeline Microservices](https://cdap.atlassian.net/wiki/spaces/DOCS/pages/975929350/Pipeline+Microservices), the `path`\nproperty is required in the request body. It's an absolute local filesystem path of a file or folder.",
    "widgets.TPFSAvro-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Basic\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Dataset Base Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"duration\",\n        \"label\": \"Duration\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"delay\",\n        \"label\": \"Delay\"\n      }\n    ]\n  }],\n  \"display-name\": \"Avro Time Partitioned Dataset\"\n}",
    "widgets.RowDenormalizer-batchaggregator": "{\n  \"outputs\": [],\n  \"metadata\": {\n    \"spec-version\": \"1.1\",\n    \"label\": \"RowDenormalizer\"\n  },\n  \"configuration-groups\": [{\n    \"label\": \"Field Mapping\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"keyField\",\n        \"label\": \"Key Field\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"nameField\",\n        \"label\": \"Input Name Field\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"valueField\",\n        \"label\": \"Input Value Field\"\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"outputFields\",\n        \"label\": \"Output fields to include\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Output Field Name\",\n          \"delimiter\": \",\"\n        }\n      },\n      {\n        \"widget-type\": \"keyvalue\",\n        \"name\": \"fieldAliases\",\n        \"label\": \"Output fields to rename\",\n        \"widget-attributes\": {\n          \"key-placeholder\": \"Field Name\",\n          \"value-placeholder\": \"New Field Name\",\n          \"showDelimiter\": \"false\"\n        }\n      }\n    ]\n  }]\n}",
    "doc.Joiner-batchjoiner": "# Joiner\n\nDescription\n-----------\nJoins records from one or more input based on join key equality.\nSupports `inner` and `outer` joins, selection and renaming of output fields.\nThe plugin is used when you want to combine fields from one or more inputs, similar to joins in SQL.\n\nProperties\n----------\n**Fields:** List of fields from each input that should be included in the output. \nOutput field names must be unique. If the same field name exists in more than one input,\neach field must be aliased (renamed) to a unique output name.\n\n**Join Type:** Type of join to perform.\nA join between two required input is an inner join. A join between a required input and an optional\ninput is a left outer join. A join between two optional inputs is an outer join.\n\nA join of more than two inputs is logically equivalent to performing inner joins over all the\nrequired inputs, followed by left outer joins on the optional inputs.\n\n**Join Condition Type:** Type of join condition to use. A condition can either be 'Basic' or 'Advanced'.\nAdvanced join conditions cannot be used in streaming pipelines or with the MapReduce engine.\nAdvanced join conditions can only be used when joining two inputs.\n\n**Join Condition:** When the condition type is 'Basic', the condition specifies the list of keys to perform the join operation.\nThe join will be performed based on equality of the join keys.\nWhen the condition type is 'Advanced', the condition can be any SQL expression supported by the engine.\nIt is important to note that advanced join conditions can be many times more expensive than basic joins performed on equality.\nAdvanced outer joins must load one of the inputs into memory.\nAdvanced inner joins do not need to load an input into memory.\nHowever, without an in-memory input, the engine will be forced to calculate a very expensive cartesian product.\n\n**Input Aliases:** When using advanced join conditions, input aliases can be specified to make the SQL expression\nmore readable. For example, if the join inputs are named 'User Signups 2020' and 'Signup Locations',\nthey can be aliased to a simpler names like 'users' and 'locations'. This allows you to use a simpler condition like \n'users.loc = locations.id or users.loc_name = locations.name'.\n\n**Inputs to Load in Memory:** Hint to the underlying execution engine that the specified input data should be\nloaded into memory to perform an in-memory join. This is ignored by the MapReduce engine and passed onto the Spark engine.\nAn in-memory join performs well when one side of the join is small (for example, under 1gb). Be sure to set\nSpark executor and driver memory to a number large enough to load all of these datasets into memory. This is most commonly\nused when a large input is being joined to a small input and will lead to much better performance in such scenarios.\nA general rule of thumb is to set executor and driver memory to fives times the dataset size.\n\n**Join on Null Keys:** Whether to join rows together if both of their key values are null.\nFor example, suppose the join is on a 'purchases' input that contains:\n\n| purchase_id | customer_name | item   |\n| ----------- | ------------- | ------ |\n| 1           | alice         | donut  |\n| 2           |               | coffee | \n| 3           | bob           | water  |\n\nand a 'customers' input that contains:\n\n| customer_id | name   |\n| ----------- | ------ |\n| 1           | alice  |\n| 2           |        |\n| 3           | bob    |\n\nThe join is a left outer join on purchases.customer_name = customers.name.\nIf this property is set to true, the joined output would be:\n\n| purchase_id | customer_name | item   | customer_id | name  |\n| ----------- | ------------- | ------ | ----------- | ----- |\n| 1           | alice         | donut  | 1           | alice |\n| 2           |               | coffee | 2           |       |\n| 3           | bob           | water  | 3           | bob   |\n\nNote that the rows with a null customer name were joined together, with customer_id set to 2 for purchase 2.\nIf this property is set to false, the joined output would be:\n\n| purchase_id | customer_name | item   | customer_id | name  |\n| ----------- | ------------- | ------ | ----------- | ----- |\n| 1           | alice         | donut  | 1           | alice |\n| 2           |               | coffee |             |       |\n| 3           | bob           | water  | 3           | bob   |\n\nIn this scenario, the null customer name on the left did not get joined to the null customer name on the right.\nTraditional relational database systems do not join on null key values.\nIn most situations, you will want to do the same and set this to false. \nSetting it to true can cause a large drop in performance if there are a lot of null keys in your input data.\n\n**Number of Partitions:** Number of partitions to use when grouping fields. \nThis value is ignored if an input is loaded into memory or if an advanced join is being performed. \nWhen an input is loaded into memory, the number of partitions will be equal\nto the number of partitions used for the input that is not loaded into memory.\nIf no value is given, or if an inner advanced join is being performed, the number of partitions will be determined\nby the value of 'spark.sql.shuffle.partitions' in the engine config, which defaults to 200. \n\n**Distribution:** Enabling distribution will increase the level of parallelism when joining skewed data. \nA skewed join happens when a significant percentage of input records have the same key. Distribution is\n possible when the following conditions are met:\n1. There are exactly two input stages. \n1. Broadcast is not enabled for either stage.\n1. The skewed input is marked as `required`.\n\n\nDistribution requires two parameters:\n1. **Distribution Size:** This controls the size of the salt that will be generated for distribution. The **Number of Partitions** \nproperty should be greater than or equal to this number for optimal results. A larger value will lead to more \nparallelism but it will also grow the size of the non-skewed dataset by this factor.\n1. **Skewed Input Stage:**  Name of the skewed input stage. The skewed input stage is the one that contains many rows that join \nto the same row in the non-skewed stage. Ex. If stage A has 10 rows that join on the same row in stage B, then\nstage A is the skewed input stage.\n\nFor more information about Distribution and data skew, please see the **Skew** section of this documentation.\n\nSkew\n----------\n### Problem\nData skew is an important characteristic to consider when implementing joins. A skewed join happens\nwhen a significant percentage of input records in one dataset have the same key and therefore join to\nthe same record in the second dataset. This is problematic due to the way the execution framework handles\njoins. At a high level, all records with matching keys are grouped into a partition, these partitions \nare distributed across the nodes in a cluster to perform the join operation. In a skewed join, one \nor more of these partitions will be significantly larger than the rest, which will result in a majority \nof the workers in the cluster remaining idle while a couple workers process the large partitions. This\nresults in poor performance since the cluster is being under utilized.\n\n### Solution 1: In-Memory Join (Spark only)\n*This option is only available if the Spark engine is used, MapReduce \ndoes not support In-Memory joins.*\n\nThe first approach for increasing performance of skewed joins is using an In-Memory join. An in-memory\njoin is a performance improvement when a large dataset is being joined with a small dataset. In this\napproach, the small dataset is loaded into memory and broadcast to workers and loaded into \nworkers memory. Once it is in memory, a join is performed by iterating through the elements of the\nlarge dataset. With this approach, data from the large dataset is NEVER shuffled. Data with the \nsame key can be joined in parallel across the cluster instead of handled only by a single worker \nproviding optimal performance. Data sets that have a small size can be used for in-memory joins. \nMake sure the total size of broadcast data does not exceed **2GB.**\n  \n### Solution 2: Distribution\nDistribution should be used when the smaller dataset cannot fit into memory. This solution solves the \ndata skew problem by adding a composite key (salt) on both the datasets and by specifying a join condition \nwith a combination of composite keys along with original keys to be joined. The addition of the \ncomposite key allows the data to be spread across more workers therefore increasing parallelism which \nincreases overall performance. The following example illustrates how salting works and how it is used for distribution:\n\nSuppose the skewed side (Stage A) has data like:\n\n| id | country |\n|----|---------|\n| 0  | us      |\n| 1  | us      |\n| 2  | us      |\n| 3  | gb      |\n\nwhere most of the data has the same value for the country. The unskewed side (Stage B) has data like:\n\n | country | code | \n | ------- | ---- | \n | us      | 1    | \n | gb      | 44   | \n\nThe join key is country. With a `Distribution Size` of 2 and `Skewed Stage` of 'Stage A', a new salt\ncolumn is added to the skewed data, where the value is a random number between 0 and 1:\n\n| id | country | salt |\n|----|---------|------|\n| 0  | us      | 0    |\n| 1  | us      | 1    |\n| 2  | us      | 0    |\n| 2  | gb      | 1    |\n\n\nThe unskewed data is exploded, where each row is becomes 2 rows, one for each value between 0 and `Distribution Size`:\n\n | country | code | salt | \n | ------- | ---- | ---- | \n | us      | 1    | 0    | \n | us      | 1    | 1    | \n | gb      | 44   | 0    | \n | gb      | 44   | 1    | \n \n The salt column is added to the join key and the join can be performed as normal. \nHowever, now the skewed key can be processed across two workers which increases the performance.\n\nExample\n-------\nThis example performs an inner join on records from ``customers`` and ``purchases`` inputs\n on customer id. It selects customer_id, name, item and price fields as the output fields.\nThis is equivalent to a SQL query like:\n\n```\nSELECT customes.id as customer_id, customers.first_name as name, purchases.item, purchases.price\nFROM customers \nINNER JOIN purchases\nON customers.id = purchases.customer_id\n```\n\nFor example, suppose the joiner receives input records from customers and purchases as below:\n\n\n| id | first_name | last_name |  street_address      |   city    | state | zipcode | phone number |\n| -- | ---------- | --------- | -------------------- | --------- | ----- | ------- | ------------ |\n| 1  | Douglas    | Williams  | 1, Vista Montana     | San Jose  | CA    | 95134   | 408-777-3214 |\n| 2  | David      | Johnson   | 3, Baypointe Parkway | Houston   | TX    | 78970   | 804-777-2341 |\n| 3  | Hugh       | Jackman   | 5, Cool Way          | Manhattan | NY    | 67263   | 708-234-2168 |\n| 4  | Walter     | White     | 3828, Piermont Dr    | Orlando   | FL    | 73498   | 201-734-7315 |\n| 5  | Frank      | Underwood | 1609 Far St.         | San Diego | CA    | 29770   | 201-506-8756 |\n| 6  | Serena     | Woods     | 123 Far St.          | Las Vegas | Nv    | 45334   | 888-605-3479 |\n\n\n| customer_id | item   | price |\n| ----------- | ------ | ----- |\n| 1           | donut  | 0.80  |\n| 1           | coffee | 2.05  |\n| 2           | donut  | 1.50  |\n| 2           | plate  | 0.50  |\n| 3           | tea    | 1.99  |\n| 5           | cookie | 0.50  |\n\nOutput records will contain inner join on customer id:\n\n| customer_id | name    | item   | price |\n| ----------- | ------- | ------ | ----- |\n| 1           | Douglas | donut  | 0.80  |\n| 1           | Douglas | coffee | 2.05  |\n| 2           | David   | donut  | 1.50  |\n| 2           | David   | plate  | 0.50  |\n| 3           | Hugh    | tea    | 1.99  |\n| 5           | Frank   | cookie | 0.50  |\n",
    "widgets.Distinct-batchaggregator": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Distinct\",\n    \"properties\": [\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"fields\",\n        \"label\": \"Fields\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Field Name\",\n          \"delimiter\": \",\"\n        },\n        \"plugin-function\": {\n          \"widget\": \"outputSchema\",\n          \"method\": \"POST\",\n          \"output-property\": \"schema\",\n          \"plugin-method\": \"outputSchema\"\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"numPartitions\",\n        \"label\": \"Number of Partitions\"\n      }\n    ]\n  }]\n}",
    "doc.TPFSAvro-batchsink": "# TimePartitionedFileSet Avro Batch Sink\n\n\nDescription\n-----------\nSink for a ``TimePartitionedFileSet`` that writes data in Avro format.\nEvery time the pipeline runs, a new partition in the ``TimePartitionedFileSet``\nwill be created based on the logical start time of the run.\nAll data for the run will be written to that partition.\n\n\nUse Case\n--------\nThis sink is used whenever you want to write to a ``TimePartitionedFileSet`` in Avro format.\nFor example, you might want to create daily snapshots of a database table by reading\nthe entire contents of the table and writing to this sink.\n\n\nProperties\n----------\n**name:** Name of the ``TimePartitionedFileSet`` to which records are written.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**schema:** The Avro schema of the record being written to the sink as a JSON Object. (Macro-enabled)\n\n**basePath:** Base path for the ``TimePartitionedFileSet``. Defaults to the name of the dataset. (Macro-enabled)\n\n**filePathFormat:** Format for the time partition, as used by ``SimpleDateFormat``.\nDefaults to formatting partitions such as ``2015-01-01/20-42.142017372000``. (Macro-enabled)\n\n**timeZone:** The string ID for the time zone to format the date in. Defaults to using UTC.\nThis setting is only used if ``filePathFormat`` is not null. (Macro-enabled)\n\n**partitionOffset:** Amount of time to subtract from the pipeline runtime to determine the output partition. Defaults to 0m.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit,\nwith 's' for seconds, 'm' for minutes, 'h' for hours, and 'd' for days.\nFor example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand the offset is set to '1d', data will be written to the partition for midnight Dec 31, 2015.\" (Macro-enabled)\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, and Deflate.\n\nExample\n-------\nThis example will write to a ``TimePartitionedFileSet`` named ``'users'``:\n\n    {\n        \"name\": \"TPFSAvro\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"filePathFormat\": \"yyyy-MM-dd/HH-mm\",\n            \"timeZone\": \"America/Los_Angeles\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }\n\nIt will write data in Avro format using the given schema. Every time the pipeline runs, a\nnew partition in the ``TimePartitionedFileSet`` will be created based on the logical start\ntime of the run with the output directory ending with the date formatted as specified. All\ndata for the run will be written to that partition compressed using the Snappy codec.\n\nFor example, if the pipeline was scheduled to run at 10:00am on January 1, 2015 in Los\nAngeles, a new partition will be created with year 2015, month 1, day 1, hour 10, and\nminute 0, and the output directory for that partition would end with ``2015-01-01/10-00``.",
    "widgets.Window-windower": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Window\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"width\",\n        \"label\": \"Width\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"slideInterval\",\n        \"label\": \"Slide Interval\"\n      }\n    ]\n  }],\n  \"display-name\": \"Windower\"\n}",
    "widgets.Deduplicate-batchaggregator": "{\n  \"outputs\": [],\n  \"metadata\": {\n    \"spec-version\": \"1.1\",\n    \"label\": \"Deduplicate Aggregator\"\n  },\n  \"configuration-groups\": [{\n    \"label\": \"General\",\n    \"properties\": [\n      {\n        \"widget-type\": \"input-field-selector\",\n        \"name\": \"uniqueFields\",\n        \"label\": \"Unique Fields\",\n        \"widget-attributes\": {\"multiselect\": \"true\"}\n      },\n      {\n        \"widget-type\": \"keyvalue-dropdown\",\n        \"name\": \"filterOperation\",\n        \"label\": \"Filter Operation\",\n        \"widget-attributes\": {\n          \"kv-delimiter\": \":\",\n          \"dropdownOptions\": [\n            \"First\",\n            \"Last\",\n            \"Max\",\n            \"Min\"\n          ],\n          \"key-placeholder\": \"Field Name\",\n          \"delimiter\": \";\",\n          \"showDelimiter\": \"false\"\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"numPartitions\",\n        \"label\": \"Number of Partitions\"\n      }\n    ]\n  }]\n}",
    "doc.Window-windower": "# Window Plugin\n\n\nDescription\n-----------\nThe Window plugin is used to window a part of a streaming pipeline.\n\n\nUse Case\n--------\nWindows are used when you wish to group smaller batches of events into larger batches of events.\nFor example, if your streaming pipeline has a batch interval of one second, you may want to accumulate\nthirty seconds of events before running an aggregation, in order to accumulate enough events to generate\nuseful aggregates. You may also want to slide the window at a slower frequency. For example, you may want to\ncalculate those aggregates over every five seconds instead of every second.\n\nProperties\n----------\n**width:** The width of the window in seconds. Must be a multiple of the batch interval.\nEach window generated will contain all the events from this many seconds.\n\n**slideInterval:** The sliding interval of the window in seconds. Must be a multiple of the batch interval.\nEvery this number of seconds, a new window will be generated.\n\nExample\n-------\nThis example creates windows of width five seconds that slides every two seconds. In order to use this\nconfiguration, the batch interval for the entire pipeline must be set to one second, as both the width\nand slide interval must be multiples of the batch interval.\n\n```json\n{\n    \"name\": \"XMLReaderBatchSource\",\n    \"plugin\":{\n        \"name\": \"Window\",\n        \"type\": \"windower\",\n        \"properties\":{\n            \"width\": \"5\",\n            \"slideInterval\": \"2\"\n        }\n    }\n}\n```\n\nWith this setup, the plugin would generate these windowed batches:\n\n| time | input | output              |\n| ---- | ----- | ------------------- |\n| 1    | x1    |                     |\n| 2    | x2    | [ - - - x1 x2 ]     |\n| 3    | x3    |                     |\n| 4    | x4    | [ - x1 x2 x3 x4 ]   |\n| 5    | x5    |                     |\n| 6    | x6    | [ x2 x3 x4 x5 x6 ]  |\n| 7    | x7    |                     |\n| 8    | x8    | [ x4 x5 x6 x7 x8 ]  |\n| 9    | x9    |                     |\n| 10   | x10   | [ x6 x7 x8 x9 x10 ] |\n",
    "widgets.KVTable-batchsink": "{\n  \"outputs\": [{\n    \"schema\": {\n      \"name\": \"etlSchemaBody\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"key\",\n          \"type\": \"bytes\"\n        },\n        {\n          \"name\": \"value\",\n          \"type\": \"bytes\"\n        }\n      ]\n    },\n    \"widget-type\": \"non-editable-schema-editor\"\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"KV Table Properties\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Table Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"key.field\",\n        \"label\": \"Key Field\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"value.field\",\n        \"label\": \"Value Field\"\n      }\n    ]\n  }],\n  \"display-name\": \"Key Value Dataset\"\n}",
    "widgets.TPFSOrc-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Time Partitioned Fileset - ORC\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Dataset Base Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"filePathFormat\",\n        \"label\": \"Partition Directory Format\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"timeZone\",\n        \"label\": \"Time Zone\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"partitionOffset\",\n        \"label\": \"Partition Offset\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"compressionCodec\",\n        \"label\": \"Compression Codec\",\n        \"widget-attributes\": {\n          \"default\": \"None\",\n          \"values\": [\n            \"None\",\n            \"Snappy\",\n            \"ZLIB\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"compressionChunkSize\",\n        \"label\": \"Compression Chunk Size\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"stripeSize\",\n        \"label\": \"Bytes per stripe\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"indexStride\",\n        \"label\": \"Rows between index entries\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"createIndex\",\n        \"label\": \"Create inline indexes\",\n        \"widget-attributes\": {\n          \"default\": \"True\",\n          \"values\": [\n            \"True\",\n            \"False\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"ORC Time Partitioned Dataset\"\n}",
    "widgets.KVTable-batchsource": "{\n  \"outputs\": [{\n    \"schema\": {\n      \"name\": \"etlSchemaBody\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"key\",\n          \"type\": \"bytes\"\n        },\n        {\n          \"name\": \"value\",\n          \"type\": \"bytes\"\n        }\n      ]\n    },\n    \"widget-type\": \"non-editable-schema-editor\"\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Basic\",\n    \"properties\": [{\n      \"widget-type\": \"dataset-selector\",\n      \"name\": \"name\",\n      \"label\": \"Table Name\"\n    }]\n  }],\n  \"display-name\": \"Key Value Dataset\"\n}",
    "doc.ErrorCollector-errortransform": "# Error Collector\n\n\nDescription\n-----------\nThe ErrorCollector plugin takes errors emitted from the previous stage and flattens them by adding\nthe error message, code, and stage to the record and outputting the result.\n\nUse Case\n--------\nThe plugin is used when you want to capture errors emitted from another stage and pass them along\nwith all the error information flattened into the record. For example, you may want to connect a sink\nto this plugin in order to store and later examine the error records.\n\nProperties\n----------\n**messageField:** The name of the error message field to use in the output schema.\nThe UI will default this to 'errMsg'. If no value is specified, the error message will be dropped.\n\n**codeField:** The name of the error code field to use in the output schema.\nThe UI will default this to 'errCode'. If no value is specified, the error code will be dropped.\n\n**stageField:** The name of the error stage field to use in the output schema.\nThe UI will default this to 'errStage'. If no value is specified, the error stage will be dropped.\n\n\nExample\n-------\nThis example adds the error message, error code, and error stage as the 'errMsg', 'errCode', and 'errStage' fields.\n\n```json\n    {\n        \"name\": \"ErrorCollector\",\n        \"type\": \"errortransform\",\n        \"properties\": {\n            \"messageField\": \"errMsg\",\n            \"codeField\": \"errCode\",\n            \"stageField\": \"errStage\"\n        }\n    }\n```\n\nFor example, suppose the plugin receives this error record:\n\n| field name | type | value  |\n| ---------- | ---- | ------ |\n| A          | int  | 10     |\n| B          | int  | 20     |\n\nwith error code 17, error message 'invalid', from stage 'parser'. It will add the error information\nto the record and output:\n\n| field name | type   | value   |\n| ---------- | ------ | ------- |\n| A          | int    | 10      |\n| B          | int    | 20      |\n| errMsg     | string | invalid |\n| errCode    | int    | 17      |\n| errStage   | string | parser  |\n",
    "doc.StructuredRecordToGenericRecord-transform": "# StructuredRecord To GenericRecord Transform\n\n\nDescription\n-----------\nTransforms a StructuredRecord into an Avro GenericRecord. \n\n``StructuredRecord`` is the Java class that all built-in plugins work with. Most\n``StructuredRecord``s can be directly converted to a ``GenericRecord``. An exception is if the\n``StructuredRecord`` contains a map field with keys that are not of type ``'string'``.\n\n\nUse Case\n--------\nThe transform is used whenever you need to use an Avro ``GenericRecord``. For example, if\nyou have a custom sink that accepts as input ``GenericRecord``s, you will use this\ntransform right before the sink.\n\n\nProperties\n----------\nThe transform does not take any properties.\n\n\nExample\n-------\n\n    {\n        \"name\": \"StructuredRecordToGenericRecord\",\n        \"type\": \"transform\",\n        \"properties\": { }\n    }\n",
    "widgets.JavaScript-transform": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"metadata\": {\"spec-version\": \"1.6\"},\n  \"configuration-groups\": [{\n    \"label\": \"JavaScript\",\n    \"properties\": [\n      {\n        \"widget-type\": \"javascript-editor\",\n        \"name\": \"script\",\n        \"label\": \"JavaScript\",\n        \"widget-attributes\": {\"default\": \"/**\\n * @summary Transforms the provided input record into zero or more output records or errors.\\n\\n * Input records are available in JavaScript code as JSON objects. \\n\\n * @param input an object that contains the input record as a JSON.   e.g. to access a field called 'total' from the input record, use input.total.\\n * @param emitter an object that can be used to emit zero or more records (using the emitter.emit() method) or errors (using the emitter.emitError() method) \\n * @param context an object that provides access to:\\n *            1. CDAP Metrics - context.getMetrics().count('output', 1);\\n *            2. CDAP Logs - context.getLogger().debug('Received a record');\\n *            3. Lookups - context.getLookup('blacklist').lookup(input.id); or\\n *            4. Runtime Arguments - context.getArguments().get('priceThreshold') \\n */ \\nfunction transform(input, emitter, context) {\\n  emitter.emit(input);\\n}\"}\n      },\n      {\n        \"widget-type\": \"json-editor\",\n        \"name\": \"lookup\",\n        \"label\": \"Lookup\"\n      }\n    ]\n  }],\n  \"emit-errors\": true,\n  \"errorDataset\": {\"errorDatasetTooltip\": \"Dataset that collects error messages from emitter. Please check reference section for usage.\"},\n  \"emit-alerts\": true\n}",
    "doc.SnapshotText-batchsink": "# Snapshot Text Batch Sink\n\n\nDescription\n-----------\nA batch sink for a PartitionedFileSet that writes snapshots of data as a new\npartition. Data is written in Text format. \n\n\nUse Case\n--------\nThis sink is used whenever you want access to a PartitionedFileSet containing exactly the\nmost recent run's data in Text format. For example, you might want to create daily\nsnapshots of a database by reading the entire contents of a table, writing to this sink,\nand then other programs can analyze the contents of the specified file. Or you might want \nto see the output of the pipeline in Text Format for easy readability.\n\n\nProperties\n----------\n**name:** Name of the PartitionedFileSet to which records are written.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**basePath:** Base path for the PartitionedFileSet. Defaults to the name of the dataset. (Macro-enabled)\n\n**fileProperties:** Advanced feature to specify any additional properties that should be used with the sink,\nspecified as a JSON object of string to string. These properties are set on the dataset if one is created.\nThe properties are also passed to the dataset at runtime as arguments. (Macro-enabled)\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015. (Macro-enabled)\n\n**delimiter:** The Delimiter used to combine the Structured Record fields. Default value is tab.\n\nExample\n-------\nThis example will write to a PartitionedFileSet named 'snapshotText'. It will write data in Text format with specified delimiter. \nEvery time the pipeline runs, the most recent run will be stored in\na new partition in the PartitionedFileSet:\n\n    {\n        \"name\": \"SnapshotText\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n        \"name\": \"snapshotText\",\n        \"basePath\": \"path/to/store/data\",\n        \"cleanPartitionsOlderThan\": \"1h\",\n        \"delimiter\": \",\"\n        }\n    }\n\n",
    "widgets.SnapshotAvro-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Snapshot Fileset - AVRO\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Snapshot Base Path\"\n      },\n      {\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileProperties\",\n        \"label\": \"FileSet Properties\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"compressionCodec\",\n        \"label\": \"Compression Codec\",\n        \"widget-attributes\": {\n          \"default\": \"None\",\n          \"values\": [\n            \"None\",\n            \"Snappy\",\n            \"Deflate\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Avro Snapshot Dataset\"\n}",
    "doc.WindowsShareCopy-action": "# Windows Share Copy\nCopies a file or files on a Microsoft Windows share to an HDFS directory.\n\n## Plugin Configuration\n\n| Configuration     | Required | Default | Description                                                           |\n| ----------------- | :------: | :-----: | --------------------------------------------------------------------- |\n| NetBios Domain Name | Yes       | `null`     | Specifies the NetBios domain name.                   |\n| NetBios Hostname      | Yes       | n/a | Specifies NetBios hostname to import files from.         |\n| NetBios Username        | Yes      | n/a     | Specifies the NetBios username to use when importing files from the Windows share  |\n| NetBios Password | Yes       | n/a     | Specifies the NetBios password |\n| NetBios Share Name | Yes       | n/a     | Specifies the NetBios share name |\n| Number of Threads | No       | `1`     | Specifies the number of parallel tasks to use when executing the copy operation |\n| Source Directory | Yes       | n/a     | Specifies the NetBios directory |\n| Destination Directory | Yes       | n/a     | The valid full HDFS destination path in the same cluster where the file or files are to be moved. If a directory is specified as a destination with a file as the source, the source file will be put into that directory. If the source is a directory, it is assumed that destination is also a directory. This plugin does not check and will not catch any inconsistency |\n| Buffer Size | No       | `4096`     | The size of the buffer to be used for copying the files. Value should be a multiple of the minimum size |\n| Overwrite | No       | `true`     | Boolean that specifies if any matching files already present in the destination should be overwritten or not |\n",
    "widgets.SnapshotParquet-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Snapshot Fileset - Parquet\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Snapshot Target Path\"\n      },\n      {\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileProperties\",\n        \"label\": \"FileSet Properties\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"compressionCodec\",\n        \"label\": \"Compression Codec\",\n        \"widget-attributes\": {\n          \"default\": \"None\",\n          \"values\": [\n            \"None\",\n            \"Snappy\",\n            \"GZip\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Parquet Snapshot Dataset\"\n}",
    "widgets.SnapshotParquet-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"dataset-selector\",\n          \"name\": \"name\",\n          \"label\": \"Dataset Name\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"basePath\",\n          \"label\": \"Snapshot Base Path\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [{\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileProperties\",\n        \"label\": \"FileSet Properties\"\n      }]\n    }\n  ],\n  \"display-name\": \"Parquet Snapshot Dataset\"\n}",
    "widgets.SnapshotText-batchsink": "{\n  \"outputs\": [],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.3\"},\n  \"configuration-groups\": [{\n    \"label\": \"Snapshot Fileset - Text\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Snapshot Base Path\"\n      },\n      {\n        \"widget-type\": \"json-editor\",\n        \"name\": \"fileProperties\",\n        \"label\": \"FileSet Properties\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"Delimiter\",\n        \"label\": \"Delimiter used to combine fields\"\n      }\n    ]\n  }]\n}",
    "widgets.File-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\"default-schema\": {\n      \"name\": \"fileRecord\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"offset\",\n          \"type\": \"long\"\n        },\n        {\n          \"name\": \"body\",\n          \"type\": \"string\"\n        }\n      ]\n    }}\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\",\n          \"widget-attributes\": {\"placeholder\": \"Name used to identify this source for lineage\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"path\",\n          \"label\": \"Path\",\n          \"widget-attributes\": {\"placeholder\": \"/path/to/input\"}\n        },\n        {\n          \"widget-type\": \"plugin-list\",\n          \"name\": \"format\",\n          \"label\": \"Format\",\n          \"widget-attributes\": {\"plugin-type\": \"validatingInputFormat\"}\n        },\n        {\n          \"widget-type\": \"number\",\n          \"name\": \"sampleSize\",\n          \"label\": \"Sample Size\",\n          \"widget-attributes\": {\n            \"default\": \"1000\",\n            \"minimum\": \"1\"\n          }\n        },\n        {\n          \"widget-type\": \"keyvalue-dropdown\",\n          \"name\": \"override\",\n          \"label\": \"Override\",\n          \"widget-attributes\": {\n            \"dropdownOptions\": [\n              \"boolean\",\n              \"bytes\",\n              \"double\",\n              \"float\",\n              \"int\",\n              \"long\",\n              \"string\",\n              \"timestamp\"\n            ],\n            \"key-placeholder\": \"Field Name\",\n            \"value-placeholder\": \"Data Type\"\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"delimiter\",\n          \"label\": \"Delimiter\",\n          \"widget-attributes\": {\"placeholder\": \"Delimiter if the format is 'delimited'\"},\n          \"plugin-function\": {\n            \"widget\": \"outputSchema\",\n            \"missing-required-fields-message\": \"Please provide path field\",\n            \"method\": \"POST\",\n            \"required-fields\": [\"path\"],\n            \"label\": \"Get Schema Value\",\n            \"plugin-method\": \"getSchema\"\n          }\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"skipHeader\",\n          \"label\": \"Skip Header\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"off\": {\n              \"label\": \"False\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"True\",\n              \"value\": \"true\"\n            }\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"maxSplitSize\",\n          \"label\": \"Maximum Split Size\",\n          \"widget-attributes\": {\"placeholder\": \"Maximum split size for each partition specified in bytes\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"fileRegex\",\n          \"label\": \"Regex Path Filter\",\n          \"widget-attributes\": {\"placeholder\": \"Regular expression for files to read\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"pathField\",\n          \"label\": \"Path Field\",\n          \"widget-attributes\": {\"placeholder\": \"Output field to contain the path of the object that was read from\"}\n        },\n        {\n          \"widget-type\": \"radio-group\",\n          \"name\": \"filenameOnly\",\n          \"label\": \"Path Filename Only\",\n          \"widget-attributes\": {\n            \"layout\": \"inline\",\n            \"default\": \"false\",\n            \"options\": [\n              {\n                \"id\": \"true\",\n                \"label\": \"True\"\n              },\n              {\n                \"id\": \"false\",\n                \"label\": \"False\"\n              }\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"radio-group\",\n          \"name\": \"recursive\",\n          \"label\": \"Read Files Recursively\",\n          \"widget-attributes\": {\n            \"layout\": \"inline\",\n            \"default\": \"false\",\n            \"options\": [\n              {\n                \"id\": \"true\",\n                \"label\": \"True\"\n              },\n              {\n                \"id\": \"false\",\n                \"label\": \"False\"\n              }\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"radio-group\",\n          \"name\": \"ignoreNonExistingFolders\",\n          \"label\": \"Allow Empty Input\",\n          \"widget-attributes\": {\n            \"layout\": \"inline\",\n            \"default\": \"false\",\n            \"options\": [\n              {\n                \"id\": \"true\",\n                \"label\": \"True\"\n              },\n              {\n                \"id\": \"false\",\n                \"label\": \"False\"\n              }\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"json-editor\",\n          \"name\": \"fileSystemProperties\",\n          \"label\": \"File System Properties\"\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"fileEncoding\",\n          \"label\": \"File encoding\",\n          \"widget-attributes\": {\n            \"default\": \"UTF-8\",\n            \"values\": [\n              {\n                \"label\": \"UTF-8\",\n                \"value\": \"UTF-8\"\n              },\n              {\n                \"label\": \"UTF-32\",\n                \"value\": \"UTF-32\"\n              },\n              {\n                \"label\": \"ISO-8859-1 (Latin-1 Western European)\",\n                \"value\": \"ISO-8859-1\"\n              },\n              {\n                \"label\": \"ISO-8859-2 (Latin-2 Central European)\",\n                \"value\": \"ISO-8859-2\"\n              },\n              {\n                \"label\": \"ISO-8859-3 (Latin-3 South European)\",\n                \"value\": \"ISO-8859-3\"\n              },\n              {\n                \"label\": \"ISO-8859-4 (Latin-4 North European)\",\n                \"value\": \"ISO-8859-4\"\n              },\n              {\n                \"label\": \"ISO-8859-5 (Latin/Cyrillic)\",\n                \"value\": \"ISO-8859-5\"\n              },\n              {\n                \"label\": \"ISO-8859-6 (Latin/Arabic)\",\n                \"value\": \"ISO-8859-6\"\n              },\n              {\n                \"label\": \"ISO-8859-7 (Latin/Greek)\",\n                \"value\": \"ISO-8859-7\"\n              },\n              {\n                \"label\": \"ISO-8859-8 (Latin/Hebrew)\",\n                \"value\": \"ISO-8859-8\"\n              },\n              {\n                \"label\": \"ISO-8859-9 (Latin-5 Turkish)\",\n                \"value\": \"ISO-8859-9\"\n              },\n              {\n                \"label\": \"ISO-8859-11 (Latin/Thai)\",\n                \"value\": \"ISO-8859-11\"\n              },\n              {\n                \"label\": \"ISO-8859-13 (Latin-7 Baltic Rim)\",\n                \"value\": \"ISO-8859-13\"\n              },\n              {\n                \"label\": \"ISO-8859-15 (Latin-9)\",\n                \"value\": \"ISO-8859-15\"\n              },\n              {\n                \"label\": \"Windows-1250\",\n                \"value\": \"Windows-1250\"\n              },\n              {\n                \"label\": \"Windows-1251\",\n                \"value\": \"Windows-1251\"\n              },\n              {\n                \"label\": \"Windows-1252\",\n                \"value\": \"Windows-1252\"\n              },\n              {\n                \"label\": \"Windows-1253\",\n                \"value\": \"Windows-1253\"\n              },\n              {\n                \"label\": \"Windows-1254\",\n                \"value\": \"Windows-1254\"\n              },\n              {\n                \"label\": \"Windows-1255\",\n                \"value\": \"Windows-1255\"\n              },\n              {\n                \"label\": \"Windows-1256\",\n                \"value\": \"Windows-1256\"\n              },\n              {\n                \"label\": \"Windows-1257\",\n                \"value\": \"Windows-1257\"\n              },\n              {\n                \"label\": \"Windows-1258\",\n                \"value\": \"Windows-1258\"\n              },\n              {\n                \"label\": \"IBM00858\",\n                \"value\": \"IBM00858\"\n              },\n              {\n                \"label\": \"IBM01140\",\n                \"value\": \"IBM01140\"\n              },\n              {\n                \"label\": \"IBM01141\",\n                \"value\": \"IBM01141\"\n              },\n              {\n                \"label\": \"IBM01142\",\n                \"value\": \"IBM01142\"\n              },\n              {\n                \"label\": \"IBM01143\",\n                \"value\": \"IBM01143\"\n              },\n              {\n                \"label\": \"IBM01144\",\n                \"value\": \"IBM01144\"\n              },\n              {\n                \"label\": \"IBM01145\",\n                \"value\": \"IBM01145\"\n              },\n              {\n                \"label\": \"IBM01146\",\n                \"value\": \"IBM01146\"\n              },\n              {\n                \"label\": \"IBM01147\",\n                \"value\": \"IBM01147\"\n              },\n              {\n                \"label\": \"IBM01148\",\n                \"value\": \"IBM01148\"\n              },\n              {\n                \"label\": \"IBM01149\",\n                \"value\": \"IBM01149\"\n              },\n              {\n                \"label\": \"IBM037\",\n                \"value\": \"IBM037\"\n              },\n              {\n                \"label\": \"IBM1026\",\n                \"value\": \"IBM1026\"\n              },\n              {\n                \"label\": \"IBM1047\",\n                \"value\": \"IBM1047\"\n              },\n              {\n                \"label\": \"IBM273\",\n                \"value\": \"IBM273\"\n              },\n              {\n                \"label\": \"IBM277\",\n                \"value\": \"IBM277\"\n              },\n              {\n                \"label\": \"IBM278\",\n                \"value\": \"IBM278\"\n              },\n              {\n                \"label\": \"IBM280\",\n                \"value\": \"IBM280\"\n              },\n              {\n                \"label\": \"IBM284\",\n                \"value\": \"IBM284\"\n              },\n              {\n                \"label\": \"IBM285\",\n                \"value\": \"IBM285\"\n              },\n              {\n                \"label\": \"IBM290\",\n                \"value\": \"IBM290\"\n              },\n              {\n                \"label\": \"IBM297\",\n                \"value\": \"IBM297\"\n              },\n              {\n                \"label\": \"IBM420\",\n                \"value\": \"IBM420\"\n              },\n              {\n                \"label\": \"IBM424\",\n                \"value\": \"IBM424\"\n              },\n              {\n                \"label\": \"IBM437\",\n                \"value\": \"IBM437\"\n              },\n              {\n                \"label\": \"IBM500\",\n                \"value\": \"IBM500\"\n              },\n              {\n                \"label\": \"IBM775\",\n                \"value\": \"IBM775\"\n              },\n              {\n                \"label\": \"IBM850\",\n                \"value\": \"IBM850\"\n              },\n              {\n                \"label\": \"IBM852\",\n                \"value\": \"IBM852\"\n              },\n              {\n                \"label\": \"IBM855\",\n                \"value\": \"IBM855\"\n              },\n              {\n                \"label\": \"IBM857\",\n                \"value\": \"IBM857\"\n              },\n              {\n                \"label\": \"IBM860\",\n                \"value\": \"IBM860\"\n              },\n              {\n                \"label\": \"IBM861\",\n                \"value\": \"IBM861\"\n              },\n              {\n                \"label\": \"IBM862\",\n                \"value\": \"IBM862\"\n              },\n              {\n                \"label\": \"IBM863\",\n                \"value\": \"IBM863\"\n              },\n              {\n                \"label\": \"IBM864\",\n                \"value\": \"IBM864\"\n              },\n              {\n                \"label\": \"IBM865\",\n                \"value\": \"IBM865\"\n              },\n              {\n                \"label\": \"IBM866\",\n                \"value\": \"IBM866\"\n              },\n              {\n                \"label\": \"IBM868\",\n                \"value\": \"IBM868\"\n              },\n              {\n                \"label\": \"IBM869\",\n                \"value\": \"IBM869\"\n              },\n              {\n                \"label\": \"IBM870\",\n                \"value\": \"IBM870\"\n              },\n              {\n                \"label\": \"IBM871\",\n                \"value\": \"IBM871\"\n              },\n              {\n                \"label\": \"IBM918\",\n                \"value\": \"IBM918\"\n              }\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"hidden\",\n          \"name\": \"copyHeader\"\n        }\n      ]\n    }\n  ]\n}",
    "widgets.TPFSAvro-batchsink": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Time Partitioned Fileset - AVRO\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Dataset Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"basePath\",\n        \"label\": \"Dataset Base Path\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"filePathFormat\",\n        \"label\": \"Partition Directory Format\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"timeZone\",\n        \"label\": \"Time Zone\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"partitionOffset\",\n        \"label\": \"Partition Offset\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"cleanPartitionsOlderThan\",\n        \"label\": \"Clean Partitions Older Than\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"compressionCodec\",\n        \"label\": \"Compression Codec\",\n        \"widget-attributes\": {\n          \"default\": \"None\",\n          \"values\": [\n            \"None\",\n            \"Snappy\",\n            \"Deflate\"\n          ]\n        }\n      }\n    ]\n  }],\n  \"display-name\": \"Avro Time Partitioned Dataset\"\n}",
    "doc.GroupByAggregate-batchaggregator": "# GroupBy Aggregate\n\n\nDescription\n-----------\nGroups by one or more fields, then performs one or more aggregate functions on each group.\nSupports `Average`, `Count`, `First`, `Last`, `Max`, `Min`,`Sum`,`Collect List`,`Collect Set`, \n`Standard Deviation`, `Variance`, `Count Distinct`, `Longest String`,`Shortest String`,`Count Nulls`,\n`Concat`, `Concat Distinct`, `Logical And`, `Logical Or`, `Sum Of Squares`, `Corrected Sum Of Squares`, \n`Any If`, `Average If`, `Count If`, `Max If`, `Min If`, `Sum If`, `Collect List If`, `Collect Set If`,\n`Standard Deviation If`, `Variance If`, `Count Distinct If`, `Longest String If`, `Shortest String If`,\n`Concat If`, `Logical And If`, `Logical Or If`, `Sum Of Squares If`, `Corrected Sum Of Squares If`\nas aggregate functions.\n\nUse Case\n--------\nThe transform is used when you want to calculate some basic aggregations in your data similar\nto what you could do with a group-by query in SQL.\n\nProperties\n----------\n**groupByFields:** Comma-separated list of fields to group by.\nRecords with the same value for all these fields will be grouped together.\nRecords output by this aggregator will contain all the group by fields and aggregate fields.\nFor example, if grouping by the ``user`` field and calculating an aggregate ``numActions:count(*)``,\noutput records will have a ``user`` field and a ``numActions`` field. (Macro-enabled)\n\n**aggregates:** Aggregates to compute on each group of records.\nSupported aggregate functions are `avg`, `count`, `count(*)`, `first`, `last`, `max`, `min`,`sum`,`collectList`,\n`collectSet`, `countDistinct`, `longestString`, `shortestString`, `countNulls`, `concat`, `variance` `concatDistinct`,\n`stdDev`,`logicalAnd`, `logicalOr`, `sumOfSquares`, `correctedSumOfSquares`, `avgIf`, `countIf`, `maxIf`, `minIf`, \n`sumIf`, `collectListIf`, `collectSetIf`, `countDistinctIf`, `longestStringIf`, `shortestStringIf`, `concatIf`,\n`varianceIf`, `anyIf`, `concatDistinctIf`, `stdDevIf` `logicalAndIf`, `logicalOrIf`, `sumOfSquaresIf`, \n`correctedSumOfSquaresIf`.\nA function must specify the field it should be applied on, as well as the name it should \nbe called. Aggregates are specified using the syntax `name:function(field)[, other aggregates]`.\nFor example, ``avgPrice:avg(price),cheapest:min(price),countPricesHigherThan:countIf(price):condition(price>500)``\nwill calculate three aggregates.\nThe first will create a field called ``avgPrice`` that is the average of all ``price`` fields in the group.\nThe second will create a field called ``cheapest`` that contains the minimum ``price`` field in the group.\nThe third will create a field ``countPricesHigherThan`` that contains the number of all ``price`` fields in the group \nthat meet the condition bigger than 500.\nThe count function differs from count(*) in that it contains non-null values of a specific field,\nwhile count(*) will count all records regardless of value. (Macro-enabled)\n\n**numPartitions:** Number of partitions to use when grouping fields. If not specified, the execution\nframework will decide on the number to use.\n\nExample\n-------\nThis example groups records by their ``user`` and ``item`` fields.\nIt then calculates three aggregates for each group. The first is a sum on ``price``,\nthe second counts the number of records in the group and third one calculates the average price of each item not taking \ninto consideration prices lower than 0.50.\n\n```json\n    {\n        \"name\": \"GroupByAggregate\",\n        \"type\": \"batchaggregator\",\n        \"properties\": {\n            \"groupByFields\": \"user,item\",\n            \"aggregates\": \"totalSpent:sum(price),numPurchased:count(*),avgItemPrice:avgIf(price):condition(price>=0.50)\"\n        }\n    }\n```\n\nFor example, suppose the aggregator receives input records where each record represents a purchase:\n\n| user  | item   | price |\n| ----- | ------ | ----- |\n| bob   | donut  | 0.80  |\n| bob   | coffee | 2.05  |\n| bob   | coffee | 0.35  |\n| bob   | donut  | 1.50  |\n| bob   | donut  | 0.50  |\n| bob   | donut  | 0.45  |\n| bob   | coffee | 3.50  |\n| alice | tea    | 1.99  |\n| alice | cookie | 0.50  |\n| alice | cookie | 0.80  |\n| alice | tea    | 1.50  |\n| alice | tea    | 0.30  |\n\nOutput records will contain all group fields in addition to a field for each aggregate:\n\n| user  | item   | totalSpent | numPurchased | avgItemPrice |\n| ----- | ------ | ---------- | ------------ | ------------ |\n| bob   | donut  | 3.25       | 4            | 0.933        |\n| bob   | coffee | 5.90       | 3            | 2.775        |\n| alice | tea    | 3.79       | 3            | 1.745        |\n| alice | cookie | 1.30       | 2            | 0.65         |\n",
    "widgets.XMLReader-batchsource": "{\n  \"outputs\": [{\n    \"schema\": {\n      \"name\": \"etlSchemaBody\",\n      \"type\": \"record\",\n      \"fields\": [\n        {\n          \"name\": \"offset\",\n          \"type\": \"long\"\n        },\n        {\n          \"name\": \"filename\",\n          \"type\": \"string\"\n        },\n        {\n          \"name\": \"record\",\n          \"type\": \"string\"\n        }\n      ]\n    },\n    \"widget-type\": \"non-editable-schema-editor\"\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Basic\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\",\n          \"widget-attributes\": {\"placeholder\": \"Name used to identify this source for lineage\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"path\",\n          \"label\": \"Path\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"nodePath\",\n          \"label\": \"Node Path\"\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"actionAfterProcess\",\n          \"label\": \"Action After Processing File\",\n          \"widget-attributes\": {\n            \"default\": \"None\",\n            \"values\": [\n              \"None\",\n              \"Delete\",\n              \"Move\",\n              \"Archive\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"reprocessingRequired\",\n          \"label\": \"Reprocessing Required\",\n          \"widget-attributes\": {\n            \"default\": \"Yes\",\n            \"values\": [\n              \"Yes\",\n              \"No\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"temporaryFolder\",\n          \"label\": \"Temporary Folder\",\n          \"widget-attributes\": {\"default\": \"/tmp\"}\n        }\n      ]\n    },\n    {\n      \"label\": \"Advanced\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"pattern\",\n          \"label\": \"File Pattern\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"targetFolder\",\n          \"label\": \"Target Folder\"\n        },\n        {\n          \"widget-type\": \"hidden\",\n          \"name\": \"tableName\",\n          \"label\": \"Table Name\"\n        },\n        {\n          \"widget-type\": \"hidden\",\n          \"name\": \"tableExpiryPeriod\",\n          \"label\": \"Table Data Expiry Period (Days)\"\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"enableExternalEntities\",\n          \"label\": \"Enable processing external entities\",\n          \"widget-attributes\": {\n            \"off\": {\n              \"label\": \"Off\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"On\",\n              \"value\": \"true\"\n            }\n          }\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"supportDTD\",\n          \"label\": \"Enable XML parser to support DTDs\",\n          \"widget-attributes\": {\n            \"off\": {\n              \"label\": \"Off\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"On\",\n              \"value\": \"true\"\n            }\n          }\n        }\n      ]\n    }\n  ]\n}",
    "doc.KVTable-batchsource": "# KeyValueTable Batch Source\n\n\nDescription\n-----------\nReads the entire contents of a KeyValueTable, outputting records with a 'key' field and a\n'value' field. Both fields are of type bytes.\n\n\nUse Case\n--------\nThe source is used whenever you need to read from a KeyValueTable in batch. For example,\nyou may want to periodically dump the contents of a KeyValueTable to a Table.\n\n\nProperties\n----------\n**Table Name:** Name of the KeyValueTable to read from. If the table does not already exist, it will be created. (Macro-enabled)\n\n\nExample\n-------\nThis example reads from a KeyValueTable named 'items':\n\n```json\n    {\n        \"name\": \"KVTable\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"name\": \"items\"\n        }\n    }\n```\n\nIt outputs records with this schema:\n\n| field name | type  |\n| ---------- | ----- |\n| key        | bytes |\n| value      | bytes |\n",
    "doc.SnapshotParquet-batchsource": "# Snapshot Parquet Batch Source\n\n\nDescription\n-----------\nA batch source that reads from a corresponding SnapshotParquet sink.\nThe source will only read the most recent snapshot written to the sink.\n\n\nUse Case\n--------\nThis source is used whenever you want to read data written to the corresponding\nSnapshotParquet sink. It will read only the last snapshot written to that sink. For\nexample, you might want to create daily snapshots of a database by reading the entire\ncontents of a table and writing it to a SnapshotParquet sink. You might then want to use\nthis source to read the most recent snapshot and run a data analysis on it.\n\n\nProperties\n----------\n**Dataset Name:** Name of the PartitionedFileSet to read from.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**Snapshot Base Path:** Base path for the PartitionedFileSet. Defaults to the name of the dataset. (Macro-enabled)\n\n**FileSet Properties:** Advanced feature to specify any additional properties that should be used with the source,\nspecified as a JSON object of string to string. These properties are set on the dataset if one is created.\nThe properties are also passed to the dataset at runtime as arguments. (Macro-enabled)\n\n**Schema:** The Parquet schema of the record being written to the sink as a JSON object.\n\n\nExample\n-------\nThis example will read from a SnapshotFileSet named 'users'. It will read data in Parquet format\nusing the given schema. Every time the pipeline runs, only the most recently added snapshot will\nbe read:\n\n    {\n        \"name\": \"SnapshotParquet\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"schema\": \"{\n              \\\"type\\\":\\\"record\\\",\n              \\\"name\\\":\\\"user\\\",\n              \\\"fields\\\":[\n                  {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                  {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                  {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n              ]\n            }\"\n        }\n    }\n",
    "widgets.Excel-batchsource": "{\n  \"outputs\": [],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Excel File Properties\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"filePath\",\n          \"label\": \"File Path\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"filePattern\",\n          \"label\": \"File Regex\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"memoryTableName\",\n          \"label\": \"File Tracking Table\"\n        },\n        {\n          \"widget-type\": \"number\",\n          \"name\": \"tableExpiryPeriod\",\n          \"label\": \"Tracking Table Data Expiry Period (Days)\"\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"reprocess\",\n          \"label\": \"Reprocess Files\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"values\": [\n              \"true\",\n              \"false\"\n            ]\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Sheet Selection\",\n      \"properties\": [\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"sheet\",\n          \"label\": \"Select Sheet Using\",\n          \"widget-attributes\": {\n            \"default\": \"Sheet Number\",\n            \"values\": [\n              \"Sheet Name\",\n              \"Sheet Number\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"sheetValue\",\n          \"label\": \"Value\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Filtering and Mapping\",\n      \"properties\": [\n        {\n          \"widget-type\": \"csv\",\n          \"name\": \"columnList\",\n          \"label\": \"Columns To Be Extracted\"\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"columnMapping\",\n          \"label\": \"Column Label Mapping \",\n          \"widget-attributes\": {\"showDelimiter\": \"false\"}\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"skipFirstRow\",\n          \"label\": \"Skip First Row\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"values\": [\n              \"true\",\n              \"false\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"terminateIfEmptyRow\",\n          \"label\": \"Terminate If Empty Row\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"values\": [\n              \"true\",\n              \"false\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"rowsLimit\",\n          \"label\": \"Max Rows Limit\"\n        },\n        {\n          \"widget-type\": \"keyvalue-dropdown\",\n          \"name\": \"outputSchema\",\n          \"label\": \"Field Name Schema Type Mapping\",\n          \"widget-attributes\": {\n            \"dropdownOptions\": [\n              \"boolean\",\n              \"bytes\",\n              \"double\",\n              \"float\",\n              \"int\",\n              \"long\",\n              \"string\"\n            ],\n            \"key-placeholder\": \"Field Name\",\n            \"showDelimiter\": \"false\"\n          }\n        }\n      ]\n    },\n    {\"properties\": [\n      {\n        \"widget-type\": \"hidden\",\n        \"name\": \"ifErrorRecord\",\n        \"label\": \"On Error\",\n        \"widget-attributes\": {\n          \"default\": \"Ignore error and continue\",\n          \"values\": [\n            \"Ignore error and continue\",\n            \"Exit on error\",\n            \"Write to error dataset\"\n          ]\n        }\n      },\n      {\n        \"widget-type\": \"hidden\",\n        \"name\": \"errorDatasetName\",\n        \"label\": \"Error Dataset\"\n      }\n    ]}\n  ]\n}",
    "widgets.Projection-transform": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Projection Configuration\",\n    \"properties\": [\n      {\n        \"widget-type\": \"keyvalue-dropdown\",\n        \"name\": \"convert\",\n        \"label\": \"Convert\",\n        \"widget-attributes\": {\n          \"dropdownOptions\": [\n            \"boolean\",\n            \"bytes\",\n            \"double\",\n            \"float\",\n            \"int\",\n            \"long\",\n            \"string\"\n          ],\n          \"key-placeholder\": \"Field Name\",\n          \"showDelimiter\": \"false\"\n        }\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"drop\",\n        \"label\": \"Fields to drop\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Field Name\",\n          \"delimiter\": \",\"\n        }\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"keep\",\n        \"label\": \"Fields to keep\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Field Name\",\n          \"delimiter\": \",\"\n        }\n      },\n      {\n        \"widget-type\": \"keyvalue\",\n        \"name\": \"rename\",\n        \"label\": \"Fields to rename\",\n        \"widget-attributes\": {\n          \"key-placeholder\": \"Field Name\",\n          \"value-placeholder\": \"New Field Name\",\n          \"showDelimiter\": \"false\"\n        }\n      }\n    ]\n  }]\n}",
    "doc.File-batchsource": "# File Batch Source\n\n\nDescription\n-----------\nThis source is used whenever you need to read from a distributed file system.\nFor example, you may want to read in log files from S3 every hour and then store\nthe logs in a TimePartitionedFileSet.\n\n\nProperties\n----------\n**Reference Name:** Name used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**Path:** Path to read from. For example, s3a://<bucket>/path/to/input\n\n**Format:** Format of the data to read.\nThe format must be one of 'avro', 'blob', 'csv', 'delimited', 'json', 'parquet', 'text', 'tsv', or the\nname of any format plugin that you have deployed to your environment.\nIf the format is a macro, only the pre-packaged formats can be used.\nIf the format is 'blob', every input file will be read into a separate record.\nThe 'blob' format also requires a schema that contains a field named 'body' of type 'bytes'.\nIf the format is 'text', the schema must contain a field named 'body' of type 'string'.\n\n**Get Schema:** Auto-detects schema from file. Supported formats are: avro, parquet, csv, delimited, tsv, blob \nand text.\nBlob - is set by default as field named 'body' of type bytes.\nText - is set by default as two fields: 'body' of type bytes and 'offset' of type 'long'.\nJSON - is not supported, user has to manually provide the output schema.\n\n**Override:** A list of columns with the corresponding data types for whom the automatic data type detection gets\n skipped. \n \n**Sample Size:** The maximum number of rows in a file that will get investigated for automatic data type detection.\n\n**Delimiter:** Delimiter to use when the format is 'delimited'. This will be ignored for other formats.\n\n**Skip Header:** Whether to skip the first line of each file. Supported formats are 'text', 'csv', 'tsv', 'delimited'.\n\n**Maximum Split Size:** Maximum size in bytes for each input partition.\nSmaller partitions will increase the level of parallelism, but will require more resources and overhead.\nThe default value is 128MB.\n\n**Regex Path Filter:** Regular expression that file paths must match in order to be included in the input.\nThe full file path is compared, not just the file name.\nIf no value is given, no file filtering will be done.\nFor example, a regex of .+\\.csv will read only files that end in '.csv'.\n\nSee https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html for more information about \nthe regular expression syntax\n\n**Path Field:** Output field to place the path of the file that the record was read from.\nIf not specified, the file path will not be included in output records.\nIf specified, the field must exist in the output schema as a string.\n\n**Path Filename Only:** Whether to only use the filename instead of the URI of the file path when a path field is given.\nThe default value is false.\n\n**Read Files Recursively:** Whether files are to be read recursively from the path. The default value is false.\n\n**Allow Empty Input:** Whether to allow an input path that contains no data. When set to false, the plugin\nwill error when there is no data to read. When set to true, no error will be thrown and zero records will be read.\n\n**File System Properties:** Additional properties to use with the InputFormat when reading the data.\n",
    "doc.Excel-batchsource": "# Excel Batch Source\n\n\nDescription\n-----------\nThe Excel plugin provides user the ability to read data from one or more Excel file(s).\n\nThe plugin supports following types of Excel file(s):\nMicrosoft Excel 97(-2007) file format\nMicrosoft Excel XML (2007+) file format\n\n\nUse Case\n--------\nThe Excel plugin is used to read excel file(s) and converts the rows to structured records based\non the column names, column-label mapping and column-type mapping provided by the user. Also keeps track\nof all the processed excel files in a memory table provided by the user. So, that if user has the option\nnot to reprocess a particular file.\n\n\nProperties\n----------\n\n**filePath:** Path of the excel file(s) to be read. (Macro-enabled) Supports below formats:\n\n      Microsoft Excel 97(-2007) file format\n      Microsoft Excel XML (2007+) file format\n\n**filePattern:** Regex pattern to select specific excel file(s) from the path provided\nin **filePath** input. (Macro-enabled)\n\n**memoryTableName:** KeyValue table name to keep the track of processed files. This can be\na new table or existing one. (Macro-enabled)\n\n**reprocess:** Specify whether the files mentioned in the memory table should be reprocessed or not.\n\n**sheet:** Specifies whether sheet has to be processed by sheet name or sheet number.\n\n**sheetValue:** Specifies the value corresponding to 'sheet' input. Value can be either actual\nsheet name or sheet number.\nfor example: 'Sheet1' or '0' in case user selects 'Sheet Name' or 'Sheet Number' as 'sheet'\ninput respectively. Sheet number starts with 0. (Macro-enabled)\n\n**columnList:** Specify the excel column names which needs to be extracted from the excel sheet.\nColumn name has to be same as excel column name; for example: A, B, etc.\n\n**columnMapping:** List of the excel column names to be renamed. The key specifies the name of the\nexcel column to be renamed, with its corresponding value specifying the new name for that column.\nColumn name has to be same as excel column name; for example: A, B, etc.\n\n**skipFirstRow:** Specify whether the first row in the excel sheet needs to be processed or not.\n\n**terminateIfEmptyRow:** Specify whether processing needs to be terminated in case an empty row is\nencountered while processing excel files.\n\n**rowsLimit:** Maximum row limit for each sheet to be processed. If, the limit is not provided then\nall the rows in the sheet will be processed. (Macro-enabled)\n\n**outputSchema:** Mapping of excel column names in the output schema to data types. Consists of\na comma-separated list. This input is mandatory if no inputs for 'columnList' has been provided.\nColumn name has to be same as excel column name; for example: A, B, etc.\n\nIf type has not been provided for a column mentioned in **columnList** input, then output data type\nof that column will be **string**.\n\n**ifErrorRecord:** Specifies the action to be take in case of an error. (Macro-enabled)\n\n**errorDatasetName:** Table name to keep the error record encountered while processing the excel file(s). (Macro-enabled)\n\n\nCondition\n---------\n\n1. To process an excel sheet, either of **columnList** or **outputSchema** is mandatory.\n2. If all the columns needs to be processed, then **columnList** or **outputSchema** can be used to specify the column\n   names.\n\n\nExample\n-------\n\nThis example reads all files with pattern **.*** from a hdfs path \"hdfs://<namenode-hostname>:9000/cdap\"  and parses it\nusing the column list, column-label mapping and column-type mapping. It also keeps track of the processed\nfile name in specified memory table. It will drop columns other than the one mentioned in **columnList** and\ngenerate structured records according to the inputs.\n\nThe plugin JSON Representation will be:\n\n```json\n    {\n      \"name\": \"Excel\",\n      \"type\": \"batchsource\",\n      \"properties\": {\n            \"filePath\": \"hdfs://<namenode-hostname>:9000/cdap\",\n            \"filePattern\": \".*\",\n            \"memoryTableName\": \"inventory-memory-table\",\n            \"reprocess\": \"false\",\n            \"sheet\": \"Sheet1\",\n            \"sheetValue\": \"-1\",\n            \"columnList\": \"A,B\",\n            \"columnMapping\": \"B:name,C:age\"\n            \"skipFirstRow\": \"false\",\n            \"terminateIfEmptyRow\": \"false\",\n            \"rowsLimit\": \"\" ,\n            \"outputSchema\": \"A:string\",\n            \"ifErrorRecord\" : \"Ignore error and continue\",\n            \"errorDatasetName\": \"\"\n       }\n    }\n```\n\nSuppose, the above **filePath** contains only one file with these input rows from **Sheet1**:\n\n|    A      |     B      |     C       |\n| --------- | ---------- | ----------- |\n| 1         | \"John\"     | 23          |\n| 2         | \"Alan\"     | 34          |\n| 3         | \"Ashley\"   | 45          |\n\nAfter, the processing has been done by the Excel plugin, the output will have these\nstructure and contents, with the 'B' and 'C' column names being replaced by the 'name' and 'age'\ncolumns respectively:\n\n|    A       |   name    |     age     |\n| ---------- | --------- | ----------- |\n| \"1\"        | \"John\"    | \"23\"        |\n| \"2\"        | \"Alan\"    | \"34\"        |\n| \"3\"        | \"Ashley\"  | \"45\"        |\n\nThe memory table **inventory-memory-table** will contain:\n\n|    key                                                 |   value          |\n| ------------------------------------------------------ | ---------------- |\n| \"hdfs://<namenode-hostname>:9000/tmp/inventory.xlsx\"   | \"1322018752992l\" |\n",
    "widgets.StructuredRecordToGenericRecord-transform": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Structured Record To Generic Record\",\n    \"properties\": []\n  }]\n}",
    "widgets.ErrorCollector-errortransform": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Error Collector Configuration\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"messageField\",\n        \"label\": \"Error Message Column Name\",\n        \"widget-attributes\": {\"default\": \"msg\"},\n        \"plugin-function\": {\n          \"widget\": \"outputSchema\",\n          \"method\": \"POST\",\n          \"plugin-method\": \"getSchema\"\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"codeField\",\n        \"label\": \"Error Code Column Name\",\n        \"widget-attributes\": {\"default\": \"code\"}\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"stageField\",\n        \"label\": \"Error Emitter Node Name\",\n        \"widget-attributes\": {\"default\": \"node\"}\n      }\n    ]\n  }]\n}",
    "doc.Projection-transform": "# Projection Transform\n\n\nDescription\n-----------\nThe Projection transform lets you drop, keep, rename, and cast fields to a different type.\nFields are first dropped based on the drop or keep field, then cast, then renamed.\n\nFor example, suppose the transform is configured to drop field 'B' and rename field 'A' to 'B'.\nIf the transform receives this input record:\n\n| field name | type | value  |\n| ---------- | ---- | ------ |\n| A          | int  | 10     |\n| B          | int  | 20     |\n\nfield 'B' will first be dropped:\n\n| field name | type | value  |\n| ---------- | ---- | ------ |\n| A          | int  | 10     |\n\nand then field 'A' will be renamed to 'B':\n\n| field name | type | value  |\n| ---------- | ---- | ------ |\n| B          | int  | 10     |\n\nSimilarly, the transfrom will first check if it should keep a field, and then rename it if configured to do so.\n\nUse Case\n--------\nThe transform is used when you need to drop fields, keep specific fields, change field types, or rename fields.\n\nFor example, you may want to rename a field from ``'timestamp'`` to ``'ts'`` because you want\nto write to a database where ``'timestamp'`` is a reserved keyword. You might want to\ndrop a field named ``'headers'`` because you know it is always empty for your particular\ndata source. Or, you might want to only keep fields named ``'ip'`` and ``'timestamp'`` and discard \nall other fields.\n\n\nProperties\n----------\n**drop:** Comma-separated list of fields to drop. For example: ``'field1,field2,field3'``.\n\n**keep:** Comma-separated list of fields to keep. For example: ``'field1,field2,field3'``.\n\nNote: Drop and keep fields cannot *both* be specified. At least one must be null or empty.\n\n**rename:** List of fields to rename. This is a comma-separated list of key-value pairs,\nwhere each pair is separated by a colon and specifies the input and output names.\n\nFor example: ``'datestr:date,timestamp:ts'`` specifies that the ``'datestr'`` field should be\nrenamed to ``'date'`` and the ``'timestamp'`` field should be renamed to ``'ts'``.\n\n**convert:** List of fields to convert to a different type. This is a comma-separated list\nof key-value pairs, where each pair is separated by a colon and specifies the field name\nand the desired type.\n\nFor example: ``'count:long,price:double'`` specifies that the ``'count'`` field should be\nconverted to a long and the ``'price'`` field should be converted to a double.\n\nOnly simple types are supported (boolean, int, long, float, double, bytes, string). Any\nsimple type can be converted to bytes or a string. Otherwise, a type can only be converted\nto a larger type. For example, an int can be converted to a long, but a long cannot be\nconverted to an int.\n\n\nExample\n-------\nThis example keeps only the ``'id'`` and ``'cost'`` fields. It also changes the type of the ``'cost'``\nfield to a double and renames it ``'price'``.\n\n```json\n{\n    \"name\": \"Projection\",\n    \"type\": \"transform\",\n    \"properties\": {\n        \"drop\": \"\",\n        \"convert\": \"cost:double\",\n        \"rename\": \"cost:price\",\n            \"keep\": \"id,cost\"\n    }\n}\n```\n \nFor example, if the transform receives this input record:\n\n| field name | type                | value                |\n| ---------- | ------------------- | -------------------- |\n| id         | string              | \"abc123\"             |\n| ts         | long                | 1234567890000        |\n| headers    | map<string, string> | { \"user\": \"samuel\" } |\n| cost       | float               | 8.88                 |\n\nIt will transform it to this output record:\n\n| field name | type                | value                |\n| ---------- | ------------------- | -------------------- |\n| id         | string              | \"abc123\"             |\n| price      | double              | 8.88                 |\n",
    "widgets.GroupByAggregate-batchaggregator": "{\n  \"outputs\": [],\n  \"metadata\": {\n    \"spec-version\": \"1.5\",\n    \"label\": \"Group by Aggregator\"\n  },\n  \"configuration-groups\": [{\n    \"label\": \"General\",\n    \"properties\": [\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"groupByFields\",\n        \"label\": \"Group by fields\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Field Name\",\n          \"delimiter\": \",\"\n        },\n        \"plugin-function\": {\n          \"widget\": \"outputSchema\",\n          \"missing-required-fields-message\": \"'Group By Fields' & 'Aggregates' properties are required to fetch schema.\",\n          \"method\": \"POST\",\n          \"required-fields\": [\n            \"groupByFields\",\n            \"aggregates\"\n          ],\n          \"output-property\": \"schema\",\n          \"plugin-method\": \"outputSchema\"\n        }\n      },\n      {\n        \"widget-type\": \"function-dropdown-with-alias\",\n        \"name\": \"aggregates\",\n        \"label\": \"Aggregates\",\n        \"widget-attributes\": {\n          \"dropdownOptions\": [\n            \"Avg\",\n            \"Count\",\n            \"First\",\n            \"Last\",\n            \"Max\",\n            \"Min\",\n            {\n              \"label\": \"Standard Deviation\",\n              \"value\": \"Stddev\"\n            },\n            \"Sum\",\n            \"Variance\",\n            {\n              \"label\": \"Collect List\",\n              \"value\": \"CollectList\"\n            },\n            {\n              \"label\": \"Collect Set\",\n              \"value\": \"CollectSet\"\n            },\n            {\n              \"label\": \"Count Distinct\",\n              \"value\": \"CountDistinct\"\n            },\n            {\n              \"label\": \"Longest String\",\n              \"value\": \"LongestString\"\n            },\n            {\n              \"label\": \"Shortest String\",\n              \"value\": \"ShortestString\"\n            },\n            {\n              \"label\": \"Number of Nulls\",\n              \"value\": \"CountNulls\"\n            },\n            {\n              \"label\": \"Concat\",\n              \"value\": \"Concat\"\n            },\n            {\n              \"label\": \"Concat Distinct\",\n              \"value\": \"ConcatDistinct\"\n            },\n            {\n              \"label\": \"Logical AND\",\n              \"value\": \"LogicalAnd\"\n            },\n            {\n              \"label\": \"Logical OR\",\n              \"value\": \"LogicalOr\"\n            },\n            {\n              \"label\": \"Sum of squares\",\n              \"value\": \"SumOfSquares\"\n            },\n            {\n              \"label\": \"Corrected sum of squares\",\n              \"value\": \"CorrectedSumOfSquares\"\n            },\n            {\n              \"label\": \"Any If\",\n              \"value\": \"AnyIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Count If\",\n              \"value\": \"CountIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Count Distinct If\",\n              \"value\": \"CountDistinctIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Avg If\",\n              \"value\": \"AvgIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Max If\",\n              \"value\": \"MaxIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Min If\",\n              \"value\": \"MinIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Longest String If\",\n              \"value\": \"LongestStringIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Shortest String If\",\n              \"value\": \"ShortestStringIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Concat If\",\n              \"value\": \"ConcatIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Concat Distinct If\",\n              \"value\": \"ConcatDistinctIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Sum If\",\n              \"value\": \"SumIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Collect List If\",\n              \"value\": \"CollectListIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Collect Set If\",\n              \"value\": \"CollectSetIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Standard Deviation If\",\n              \"value\": \"StddevIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Variance If\",\n              \"value\": \"VarianceIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Corrected sum of squares If\",\n              \"value\": \"CorrectedSumOfSquaresIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Sum of squares If\",\n              \"value\": \"SumOfSquaresIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Logical AND If\",\n              \"value\": \"LogicalAndIf\",\n              \"hasCondition\": true\n            },\n            {\n              \"label\": \"Logical OR If\",\n              \"value\": \"LogicalOrIf\",\n              \"hasCondition\": true\n            }\n          ],\n          \"placeholders\": {\n            \"condition\": \"condition\",\n            \"field\": \"field\",\n            \"alias\": \"alias\"\n          }\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"numPartitions\",\n        \"label\": \"Number of Partitions\"\n      }\n    ]\n  }],\n  \"display-name\": \"Group By\"\n}",
    "doc.File-batchsink": "# File Sink\n\n\nDescription\n-----------\nWrites to a filesystem in various formats format.\n\nFor the csv, delimited, and tsv formats, each record is written out as delimited text.\nComplex types like arrays, maps, and records will be converted to strings using their\n``toString()`` Java method, so for practical use, fields should be limited to the\nstring, long, int, double, float, and boolean types.\n\nAll types are supported when using the avro or parquet format.\n\nProperties\n----------\n**Reference Name:** Name used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**Path:** Path to write to. For example, /path/to/output\n\n**Path Suffix:** Time format for the output directory that will be appended to the path.\nFor example, the format 'yyyy-MM-dd-HH-mm' will result in a directory of the form '2015-01-01-20-42'.\nIf not specified, nothing will be appended to the path.\"\n\n**Format:** Format to write the records in.\nThe format must be one of 'json', 'avro', 'parquet', 'csv', 'tsv', 'delimited', or the name of any\nformat plugins deployed to your environment. If the format is a macro, only the pre-packaged\nformats can be used.\n\n**Delimiter:** Delimiter to use if the format is 'delimited'.\n\n**Write Header:** Whether to write a header to each file if the format is 'delimited', 'csv', or 'tsv'.\n\n**File System Properties:** Additional properties to use with the OutputFormat when reading the data.\n",
    "doc.FileMove-action": "# File Move Action\n\n\nDescription\n-----------\nMoves a file or files.\n\n\nUse Case\n--------\nThis action can be used when a file or files need to be moved to a new location in a file system,\noften required when archiving files. The source and destination must be on the same file system.\n\n\nProperties\n----------\n**sourcePath:** The full path of the file or directory that is to be moved. In the case of a directory, if\nfileRegex is set, then only files in the source directory matching the wildcard regex will be moved.\nOtherwise, all files in the directory will be moved. For example: `hdfs://hostname/tmp`.\n\n**destPath:** The valid, full destination path in the same cluster where the file or files are to be moved.\nIf a directory is specified with a file sourcePath, the file will be put into that directory. If sourcePath is\na directory, it is assumed that destPath is also a directory.\n\n**fileRegex:** Wildcard regular expression to filter the files in the source directory that will be moved.\n\n**continueOnError:** Indicates if the pipeline should continue if the move process fails. If all files are not \nsuccessfully moved, the action will not return the files already moved to their original locations.\n\n\nExample\n-------\nThis example moves a file from `/source/path` to `/dest/path`:\n\n    {\n        \"name\": \"FileMove\",\n        \"plugin\": {\n            \"name\": \"FileMove\",\n            \"type\": \"action\",\n            \"artifact\": {\n                \"name\": \"core-plugins\",\n                \"version\": \"1.4.0-SNAPSHOT\",\n                \"scope\": \"SYSTEM\"\n            },\n            \"properties\": {\n                \"sourcePath\": \"hdfs://example.com:8020/source/path\",\n                \"destPath\": \"hdfs://example.com:8020/dest/path\",\n                \"fileRegex\": \".*\\.txt\",\n                \"continueOnError\": \"false\"\n            }\n        }\n    }\n",
    "doc.SnapshotAvro-batchsink": "# Snapshot Avro Batch Sink\n\n\nDescription\n-----------\nA batch sink for a PartitionedFileSet that writes snapshots of data as a new\npartition. Data is written in Avro format. A corresponding SnapshotAvro source\ncan be used to read only the most recently written snapshot.\n\n\nUse Case\n--------\nThis sink is used whenever you want access to a PartitionedFileSet containing exactly the\nmost recent run's data in Avro format. For example, you might want to create daily\nsnapshots of a database by reading the entire contents of a table, writing to this sink,\nand then other programs can analyze the contents of the specified file.\n\n\nProperties\n----------\n**name:** Name of the PartitionedFileSet to which records are written.\nIf it doesn't exist, it will be created. (Macro-enabled)\n\n**schema:** The Avro schema of the record being written to the sink as a JSON object. (Macro-enabled)\n\n**basePath:** Base path for the PartitionedFileSet. Defaults to the name of the dataset. (Macro-enabled)\n\n**fileProperties:** Advanced feature to specify any additional properties that should be used with the sink,\nspecified as a JSON object of string to string. These properties are set on the dataset if one is created.\nThe properties are also passed to the dataset at runtime as arguments. (Macro-enabled)\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, and Deflate.\n\nExample\n-------\nThis example will write to a PartitionedFileSet named 'users'. It will write data in Avro format\nusing the given schema. Every time the pipeline runs, the most recent run will be stored in\na new partition in the PartitionedFileSet:\n\n    {\n        \"name\": \"SnapshotAvro\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }",
    "widgets.Table-batchsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\",\n        \"map<string, string>\"\n      ],\n      \"schema-default-type\": \"string\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"name\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [{\n    \"label\": \"Basic\",\n    \"properties\": [\n      {\n        \"widget-type\": \"dataset-selector\",\n        \"name\": \"name\",\n        \"label\": \"Name\"\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"schema.row.field\",\n        \"label\": \"Row Field\"\n      }\n    ]\n  }],\n  \"display-name\": \"CDAP Table Dataset\"\n}",
    "widgets.WindowsShareCopy-action": "{\n  \"outputs\": [],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"NetBios NTLM Properties\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"netBiosDomainName\",\n          \"label\": \"Domain Name\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"netBiosHostname\",\n          \"label\": \"Hostname\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"netBiosUsername\",\n          \"label\": \"Username\"\n        },\n        {\n          \"widget-type\": \"password\",\n          \"name\": \"netBiosPassword\",\n          \"label\": \"Password\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Window Share Properties\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"netBiosSharename\",\n          \"label\": \"Share Name\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"sourceDirectory\",\n          \"label\": \"Source Directory\"\n        }\n      ]\n    },\n    {\n      \"label\": \"HDFS Properties\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"destinationDirectory\",\n          \"label\": \"Destination Directory\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"bufferSize\",\n          \"label\": \"Buffer Size\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Plugin Properties\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"numThreads\",\n          \"label\": \"Number of parallel tasks\"\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"overwrite\",\n          \"label\": \"Overwrite files\",\n          \"widget-attributes\": {\n            \"default\": \"true\",\n            \"values\": [\n              \"true\",\n              \"false\"\n            ]\n          }\n        }\n      ]\n    }\n  ]\n}",
    "doc.TPFSOrc-batchsink": "# TimePartitionedFileSet ORC Batch Sink\n\n\nDescription\n-----------\nSink for a ``TimePartitionedFileSet`` that writes data in ORC format.\nEvery time the pipeline runs, a new partition in the ``TimePartitionedFileSet``\nwill be created based on the logical start time of the run.\nAll data for the run will be written to that partition.\n\nUse Case\n--------\nThis sink is used whenever you want to write to a ``TimePartitionedFileSet`` in ORC format.\nFor example, you might want to create daily snapshots of a database table by reading\nthe entire contents of the table and writing to this sink.\n\n\nProperties\n----------\n**name:** Name of the ``TimePartitionedFileSet`` to which records are written.\nIf it doesn't exist, it will be created.\n\n**schema:** The schema of the record being written to the sink as a JSON Object. (Macro-enabled)\n\n**basePath:** Base path for the ``TimePartitionedFileSet``. Defaults to the name of the dataset.\n\n**filePathFormat:** Format for the time partition, as used by ``SimpleDateFormat``.\nDefaults to formatting partitions such as ``2015-01-01/20-42.142017372000``.\n\n**timeZone:** The string ID for the time zone to format the date in. Defaults to using UTC.\nThis setting is only used if ``filePathFormat`` is not null.\n\n**partitionOffset:** Amount of time to subtract from the pipeline runtime to determine the output partition. Defaults to 0m.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit,\nwith 's' for seconds, 'm' for minutes, 'h' for hours, and 'd' for days.\nFor example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand the offset is set to '1d', data will be written to the partition for midnight Dec 31, 2015.\"\n\n**cleanPartitionsOlderThan:** Optional property that configures the sink to delete partitions older than a specified date-time after a successful run.\nIf set, when a run successfully finishes, the sink will subtract this amount of time from the runtime and delete any partitions for time partitions older than that.\nThe format is expected to be a number followed by an 's', 'm', 'h', or 'd' specifying the time unit, with 's' for seconds,\n'm' for minutes, 'h' for hours, and 'd' for days. For example, if the pipeline is scheduled to run at midnight of January 1, 2016,\nand this property is set to 7d, the sink will delete any partitions for time partitions older than midnight Dec 25, 2015.\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, ZLIB.\n\n**compressionChunkSize** Required if setting compressionCodec. Number of bytes in each compression chunk.\n\n**stripeSize** Required if setting compressionCodec. Number of bytes in each stripe\n\n**indexStride** Required if setting compressionCodec. Number of rows between index entries (must be >= 1,000)\n\n**createIndex** Required if setting compressionCodec. Whether to create inline indexes\n\nExample\n-------\nThis example will write to a ``TimePartitionedFileSet`` named ``'users'``:\n\n    {\n        \"name\": \"TPFSOrc\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"name\": \"users\",\n            \"filePathFormat\": \"yyyy-MM-dd/HH-mm\",\n            \"timeZone\": \"America/Los_Angeles\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"string\\\"}\n                ]\n            }\"\n        }\n    }\n\nIt will write data in ORC format using the given schema. Every time the pipeline runs, a\nnew partition in the ``TimePartitionedFileSet`` will be created based on the logical start\ntime of the run with the output directory ending with the date formatted as specified. All\ndata for the run will be written to that partition.\n\nFor example, if the pipeline was scheduled to run at 10:00am on January 1, 2015 in Los\nAngeles, a new partition will be created with year 2015, month 1, day 1, hour 10, and\nminute 0, and the output directory for that partition would end with ``2015-01-01/10-00``.",
    "doc.LogParser-transform": "# Log Parser Transform\n\n\nDescription\n-----------\nParses logs from any input source for relevant information such as URI, IP,\nbrowser, device, HTTP status code, and timestamp.\n\n\nUse Case\n--------\nThis transform is used when you need to parse log entries. For example, you may\nwant to read in log files from S3 using S3Batchsource, parse the logs using\nLogParserTransform, and then store the IP and URI information in a Cube dataset.\n\n\nProperties\n----------\n**logFormat:** Log format to parse. Currently supports ``S3``, ``CLF``, and ``Cloudfront`` formats.\n\n**inputName:** Name of the field in the input schema which encodes the\nlog information. The given field must be of type ``String`` or ``Bytes``.\n\nConditions\n----------\nIf error dataset is configured, then all the erroneous rows, if present in the input, will be committed to the\nspecified error dataset.\nIf no error dataset is configured, then pipeline will get completed but with warnings in the logs.\n\nExample\n-------\nThis example searches for an input Schema field named 'body', and then attempts to parse\nthe Combined Log Format entries found in the field for the URI, IP, browser, device,\nHTTP status code, and timestamp:\n\n```json\n{\n    \"name\": \"LogParser\",\n    \"type\": \"transform\",\n    \"properties\": {\n        \"logFormat\": \"CLF\",\n        \"inputName\": \"body\"\n    }\n}\n```\n\nThe Transform will emit records with this schema:\n\n    \n| field name    | type       |\n| ------------- | ---------- |\n| uri           | string     |\n| ip            | string     |\n| browser       | string     |\n| device        | string     |\n| httpStatus    | int        |\n| ts            | long       |\n"
  },
  "parents": [
    "system:cdap-data-pipeline[6.5.0,7.0.0-SNAPSHOT)",
    "system:cdap-data-streams[6.5.0,7.0.0-SNAPSHOT)"
  ]
}